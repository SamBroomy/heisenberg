{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "from loguru import logger\n",
    "from typing import Literal\n",
    "import polars.selectors as cs\n",
    "import kuzu as kz\n",
    "from pathlib import Path\n",
    "from typing import Type, Callable\n",
    "from usearch.index import Index, Matches\n",
    "import geocoder\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "from numpy.typing import NDArray\n",
    "import duckdb\n",
    "from duckdb import DuckDBPyConnection\n",
    "from time import time\n",
    "import tantivy\n",
    "from result import Result, Ok, Err\n",
    "import json\n",
    "import re\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<duckdb.duckdb.DuckDBPyConnection at 0x109029d70>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duck_db_path = Path(\"./data/db/duck_db/data.db\")\n",
    "duck_db_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "con = duckdb.connect(database=duck_db_path.as_posix())\n",
    "\n",
    "con.execute(\"SET enable_progress_bar = false;\")\n",
    "con.install_extension(\"spatial\")\n",
    "con.load_extension(\"spatial\")\n",
    "\n",
    "# Set DuckDB optimizations\n",
    "con.execute(\"PRAGMA memory_limit='16GB'\")  # Adjust based on your system\n",
    "con.execute(\"PRAGMA threads=8\")  # Adjust based on your CPU cores\n",
    "con.execute(\"PRAGMA enable_object_cache=true\")  # Improve query caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (0, 1)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>name</th></tr><tr><td>str</td></tr></thead><tbody></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (0, 1)\n",
       "┌──────┐\n",
       "│ name │\n",
       "│ ---  │\n",
       "│ str  │\n",
       "╞══════╡\n",
       "└──────┘"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con.execute(\"SHOW TABLES\").pl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SQL_FOLDER = Path(\"./sql\")\n",
    "\n",
    "\n",
    "def sql_file(sql_path: Path | str, **kwargs) -> str:\n",
    "    if isinstance(sql_path, str):\n",
    "        sql_path = Path(sql_path)\n",
    "    if not sql_path.exists():\n",
    "        sql_path = SQL_FOLDER / sql_path\n",
    "        if not sql_path.exists():\n",
    "            raise FileNotFoundError(f\"SQL file {sql_path} not found\")\n",
    "    sql = sql_path.read_text()\n",
    "    if kwargs:\n",
    "        sql = sql.format(**kwargs)\n",
    "\n",
    "    # Validate no {kwarg} left in string (regex)\n",
    "    # if uninit_kwargs := re.findall(r\"\\{.*\\}\", sql):\n",
    "    #     raise ValueError(\n",
    "    #         f\"SQL file {sql_path} still has unprocessed kwargs: {list(set(uninit_kwargs))} in:\\n\\n{sql}\"\n",
    "    #     )\n",
    "\n",
    "    return sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-04-29 14:01:41.066\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_file\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mLoading './data/raw/geonames/allCountries.txt'...\u001b[0m\n",
      "\u001b[32m2025-04-29 14:01:41.068\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_file\u001b[0m:\u001b[36m51\u001b[0m - \u001b[34m\u001b[1mScan time: 0.001507s\u001b[0m\n",
      "\u001b[32m2025-04-29 14:01:48.194\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_file\u001b[0m:\u001b[36m58\u001b[0m - \u001b[34m\u001b[1mCollect time: 7.125114s\u001b[0m\n",
      "\u001b[32m2025-04-29 14:01:53.459\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_file\u001b[0m:\u001b[36m65\u001b[0m - \u001b[34m\u001b[1mWrite time: 5.265143s\u001b[0m\n",
      "\u001b[32m2025-04-29 14:01:58.394\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_file\u001b[0m:\u001b[36m79\u001b[0m - \u001b[34m\u001b[1mCreate time: 4.934032s\u001b[0m\n",
      "\u001b[32m2025-04-29 14:02:34.237\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_file\u001b[0m:\u001b[36m83\u001b[0m - \u001b[34m\u001b[1mCommit time: 35.843225s\u001b[0m\n",
      "\u001b[32m2025-04-29 14:02:34.238\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_file\u001b[0m:\u001b[36m86\u001b[0m - \u001b[34m\u001b[1mAnalyze time: 0.000289s\u001b[0m\n",
      "\u001b[32m2025-04-29 14:02:34.238\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_file\u001b[0m:\u001b[36m96\u001b[0m - \u001b[1mTotal time: 53.172962s\u001b[0m\n",
      "\u001b[32m2025-04-29 14:02:34.276\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_file\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mLoading './data/raw/geonames/allCountriesPostCode.txt'...\u001b[0m\n",
      "\u001b[32m2025-04-29 14:02:34.276\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_file\u001b[0m:\u001b[36m51\u001b[0m - \u001b[34m\u001b[1mScan time: 0.000212s\u001b[0m\n",
      "\u001b[32m2025-04-29 14:02:34.646\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_file\u001b[0m:\u001b[36m58\u001b[0m - \u001b[34m\u001b[1mCollect time: 0.369503s\u001b[0m\n",
      "\u001b[32m2025-04-29 14:02:34.903\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_file\u001b[0m:\u001b[36m65\u001b[0m - \u001b[34m\u001b[1mWrite time: 0.256451s\u001b[0m\n",
      "\u001b[32m2025-04-29 14:02:36.414\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_file\u001b[0m:\u001b[36m79\u001b[0m - \u001b[34m\u001b[1mCreate time: 1.510270s\u001b[0m\n",
      "\u001b[32m2025-04-29 14:02:36.439\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_file\u001b[0m:\u001b[36m83\u001b[0m - \u001b[34m\u001b[1mCommit time: 0.024611s\u001b[0m\n",
      "\u001b[32m2025-04-29 14:02:36.439\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_file\u001b[0m:\u001b[36m86\u001b[0m - \u001b[34m\u001b[1mAnalyze time: 0.000184s\u001b[0m\n",
      "\u001b[32m2025-04-29 14:02:36.440\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_file\u001b[0m:\u001b[36m96\u001b[0m - \u001b[1mTotal time: 2.164070s\u001b[0m\n",
      "\u001b[32m2025-04-29 14:02:36.445\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_file\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mLoading './data/raw/geonames/admin1CodesASCII.txt'...\u001b[0m\n",
      "\u001b[32m2025-04-29 14:02:36.446\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_file\u001b[0m:\u001b[36m51\u001b[0m - \u001b[34m\u001b[1mScan time: 0.000227s\u001b[0m\n",
      "\u001b[32m2025-04-29 14:02:36.448\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_file\u001b[0m:\u001b[36m58\u001b[0m - \u001b[34m\u001b[1mCollect time: 0.002075s\u001b[0m\n",
      "\u001b[32m2025-04-29 14:02:36.450\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_file\u001b[0m:\u001b[36m65\u001b[0m - \u001b[34m\u001b[1mWrite time: 0.001632s\u001b[0m\n",
      "\u001b[32m2025-04-29 14:02:36.521\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_file\u001b[0m:\u001b[36m79\u001b[0m - \u001b[34m\u001b[1mCreate time: 0.070535s\u001b[0m\n",
      "\u001b[32m2025-04-29 14:02:36.525\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_file\u001b[0m:\u001b[36m83\u001b[0m - \u001b[34m\u001b[1mCommit time: 0.002825s\u001b[0m\n",
      "\u001b[32m2025-04-29 14:02:36.525\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_file\u001b[0m:\u001b[36m86\u001b[0m - \u001b[34m\u001b[1mAnalyze time: 0.000241s\u001b[0m\n",
      "\u001b[32m2025-04-29 14:02:36.525\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_file\u001b[0m:\u001b[36m96\u001b[0m - \u001b[1mTotal time: 0.080180s\u001b[0m\n",
      "\u001b[32m2025-04-29 14:02:36.529\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_file\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mLoading './data/raw/geonames/admin2Codes.txt'...\u001b[0m\n",
      "\u001b[32m2025-04-29 14:02:36.530\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_file\u001b[0m:\u001b[36m51\u001b[0m - \u001b[34m\u001b[1mScan time: 0.000188s\u001b[0m\n",
      "\u001b[32m2025-04-29 14:02:36.541\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_file\u001b[0m:\u001b[36m58\u001b[0m - \u001b[34m\u001b[1mCollect time: 0.010818s\u001b[0m\n",
      "\u001b[32m2025-04-29 14:02:36.555\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_file\u001b[0m:\u001b[36m65\u001b[0m - \u001b[34m\u001b[1mWrite time: 0.013497s\u001b[0m\n",
      "\u001b[32m2025-04-29 14:02:36.652\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_file\u001b[0m:\u001b[36m79\u001b[0m - \u001b[34m\u001b[1mCreate time: 0.096483s\u001b[0m\n",
      "\u001b[32m2025-04-29 14:02:36.685\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_file\u001b[0m:\u001b[36m83\u001b[0m - \u001b[34m\u001b[1mCommit time: 0.033048s\u001b[0m\n",
      "\u001b[32m2025-04-29 14:02:36.686\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_file\u001b[0m:\u001b[36m86\u001b[0m - \u001b[34m\u001b[1mAnalyze time: 0.000163s\u001b[0m\n",
      "\u001b[32m2025-04-29 14:02:36.686\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_file\u001b[0m:\u001b[36m96\u001b[0m - \u001b[1mTotal time: 0.156918s\u001b[0m\n",
      "\u001b[32m2025-04-29 14:02:36.691\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_file\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mLoading './data/raw/geonames/adminCode5.txt'...\u001b[0m\n",
      "\u001b[32m2025-04-29 14:02:36.881\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_file\u001b[0m:\u001b[36m51\u001b[0m - \u001b[34m\u001b[1mScan time: 0.189636s\u001b[0m\n",
      "\u001b[32m2025-04-29 14:02:45.645\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_file\u001b[0m:\u001b[36m58\u001b[0m - \u001b[34m\u001b[1mCollect time: 8.763453s\u001b[0m\n",
      "\u001b[32m2025-04-29 14:02:45.651\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_file\u001b[0m:\u001b[36m65\u001b[0m - \u001b[34m\u001b[1mWrite time: 0.005566s\u001b[0m\n",
      "\u001b[32m2025-04-29 14:02:45.706\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_file\u001b[0m:\u001b[36m79\u001b[0m - \u001b[34m\u001b[1mCreate time: 0.055166s\u001b[0m\n",
      "\u001b[32m2025-04-29 14:02:45.730\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_file\u001b[0m:\u001b[36m83\u001b[0m - \u001b[34m\u001b[1mCommit time: 0.023336s\u001b[0m\n",
      "\u001b[32m2025-04-29 14:02:45.731\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_file\u001b[0m:\u001b[36m86\u001b[0m - \u001b[34m\u001b[1mAnalyze time: 0.000216s\u001b[0m\n",
      "\u001b[32m2025-04-29 14:02:45.731\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_file\u001b[0m:\u001b[36m96\u001b[0m - \u001b[1mTotal time: 9.040394s\u001b[0m\n",
      "\u001b[32m2025-04-29 14:02:45.735\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_file\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mLoading './data/raw/geonames/alternateNamesV2.txt'...\u001b[0m\n",
      "\u001b[32m2025-04-29 14:02:45.910\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_file\u001b[0m:\u001b[36m51\u001b[0m - \u001b[34m\u001b[1mScan time: 0.175163s\u001b[0m\n",
      "\u001b[32m2025-04-29 14:02:56.638\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_file\u001b[0m:\u001b[36m58\u001b[0m - \u001b[34m\u001b[1mCollect time: 10.727027s\u001b[0m\n",
      "\u001b[32m2025-04-29 14:02:59.124\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_file\u001b[0m:\u001b[36m65\u001b[0m - \u001b[34m\u001b[1mWrite time: 2.485466s\u001b[0m\n",
      "\u001b[32m2025-04-29 14:03:08.291\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_file\u001b[0m:\u001b[36m79\u001b[0m - \u001b[34m\u001b[1mCreate time: 9.166244s\u001b[0m\n",
      "\u001b[32m2025-04-29 14:03:27.227\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_file\u001b[0m:\u001b[36m83\u001b[0m - \u001b[34m\u001b[1mCommit time: 18.935397s\u001b[0m\n",
      "\u001b[32m2025-04-29 14:03:27.227\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_file\u001b[0m:\u001b[36m86\u001b[0m - \u001b[34m\u001b[1mAnalyze time: 0.000236s\u001b[0m\n",
      "\u001b[32m2025-04-29 14:03:27.227\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_file\u001b[0m:\u001b[36m96\u001b[0m - \u001b[1mTotal time: 41.492447s\u001b[0m\n",
      "\u001b[32m2025-04-29 14:03:27.258\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_file\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mLoading './data/raw/geonames/countryInfo.txt'...\u001b[0m\n",
      "\u001b[32m2025-04-29 14:03:27.258\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_file\u001b[0m:\u001b[36m51\u001b[0m - \u001b[34m\u001b[1mScan time: 0.000246s\u001b[0m\n",
      "\u001b[32m2025-04-29 14:03:27.268\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_file\u001b[0m:\u001b[36m58\u001b[0m - \u001b[34m\u001b[1mCollect time: 0.009083s\u001b[0m\n",
      "\u001b[32m2025-04-29 14:03:27.270\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_file\u001b[0m:\u001b[36m65\u001b[0m - \u001b[34m\u001b[1mWrite time: 0.001748s\u001b[0m\n",
      "\u001b[32m2025-04-29 14:03:27.273\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_file\u001b[0m:\u001b[36m79\u001b[0m - \u001b[34m\u001b[1mCreate time: 0.002932s\u001b[0m\n",
      "\u001b[32m2025-04-29 14:03:27.274\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_file\u001b[0m:\u001b[36m83\u001b[0m - \u001b[34m\u001b[1mCommit time: 0.000752s\u001b[0m\n",
      "\u001b[32m2025-04-29 14:03:27.275\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_file\u001b[0m:\u001b[36m86\u001b[0m - \u001b[34m\u001b[1mAnalyze time: 0.000207s\u001b[0m\n",
      "\u001b[32m2025-04-29 14:03:27.275\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_file\u001b[0m:\u001b[36m96\u001b[0m - \u001b[1mTotal time: 0.017244s\u001b[0m\n",
      "\u001b[32m2025-04-29 14:03:27.278\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_file\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mLoading './data/raw/geonames/featureCodes_en.txt'...\u001b[0m\n",
      "\u001b[32m2025-04-29 14:03:27.278\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_file\u001b[0m:\u001b[36m51\u001b[0m - \u001b[34m\u001b[1mScan time: 0.000167s\u001b[0m\n",
      "\u001b[32m2025-04-29 14:03:27.280\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_file\u001b[0m:\u001b[36m58\u001b[0m - \u001b[34m\u001b[1mCollect time: 0.001483s\u001b[0m\n",
      "\u001b[32m2025-04-29 14:03:27.282\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_file\u001b[0m:\u001b[36m65\u001b[0m - \u001b[34m\u001b[1mWrite time: 0.001060s\u001b[0m\n",
      "\u001b[32m2025-04-29 14:03:27.285\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_file\u001b[0m:\u001b[36m79\u001b[0m - \u001b[34m\u001b[1mCreate time: 0.003570s\u001b[0m\n",
      "\u001b[32m2025-04-29 14:03:27.287\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_file\u001b[0m:\u001b[36m83\u001b[0m - \u001b[34m\u001b[1mCommit time: 0.000761s\u001b[0m\n",
      "\u001b[32m2025-04-29 14:03:27.287\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_file\u001b[0m:\u001b[36m86\u001b[0m - \u001b[34m\u001b[1mAnalyze time: 0.000241s\u001b[0m\n",
      "\u001b[32m2025-04-29 14:03:27.288\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_file\u001b[0m:\u001b[36m96\u001b[0m - \u001b[1mTotal time: 0.009614s\u001b[0m\n",
      "\u001b[32m2025-04-29 14:03:27.291\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_file\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mLoading './data/raw/geonames/hierarchy.txt'...\u001b[0m\n",
      "\u001b[32m2025-04-29 14:03:27.521\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_file\u001b[0m:\u001b[36m51\u001b[0m - \u001b[34m\u001b[1mScan time: 0.229964s\u001b[0m\n",
      "\u001b[32m2025-04-29 14:03:46.039\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_file\u001b[0m:\u001b[36m58\u001b[0m - \u001b[34m\u001b[1mCollect time: 18.517764s\u001b[0m\n",
      "\u001b[32m2025-04-29 14:03:46.060\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_file\u001b[0m:\u001b[36m65\u001b[0m - \u001b[34m\u001b[1mWrite time: 0.019963s\u001b[0m\n",
      "\u001b[32m2025-04-29 14:03:47.046\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_file\u001b[0m:\u001b[36m79\u001b[0m - \u001b[34m\u001b[1mCreate time: 0.986077s\u001b[0m\n",
      "\u001b[32m2025-04-29 14:03:48.469\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_file\u001b[0m:\u001b[36m83\u001b[0m - \u001b[34m\u001b[1mCommit time: 1.422577s\u001b[0m\n",
      "\u001b[32m2025-04-29 14:03:48.470\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_file\u001b[0m:\u001b[36m86\u001b[0m - \u001b[34m\u001b[1mAnalyze time: 0.000262s\u001b[0m\n",
      "\u001b[32m2025-04-29 14:03:48.470\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_file\u001b[0m:\u001b[36m96\u001b[0m - \u001b[1mTotal time: 21.179638s\u001b[0m\n",
      "\u001b[32m2025-04-29 14:03:48.918\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_file\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mLoading './data/raw/geonames/iso-languagecodes.txt'...\u001b[0m\n",
      "\u001b[32m2025-04-29 14:03:48.919\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_file\u001b[0m:\u001b[36m51\u001b[0m - \u001b[34m\u001b[1mScan time: 0.000205s\u001b[0m\n",
      "\u001b[32m2025-04-29 14:03:48.921\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_file\u001b[0m:\u001b[36m58\u001b[0m - \u001b[34m\u001b[1mCollect time: 0.002012s\u001b[0m\n",
      "\u001b[32m2025-04-29 14:03:48.923\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_file\u001b[0m:\u001b[36m65\u001b[0m - \u001b[34m\u001b[1mWrite time: 0.001967s\u001b[0m\n",
      "\u001b[32m2025-04-29 14:03:48.931\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_file\u001b[0m:\u001b[36m79\u001b[0m - \u001b[34m\u001b[1mCreate time: 0.007211s\u001b[0m\n",
      "\u001b[32m2025-04-29 14:03:48.932\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_file\u001b[0m:\u001b[36m83\u001b[0m - \u001b[34m\u001b[1mCommit time: 0.001337s\u001b[0m\n",
      "\u001b[32m2025-04-29 14:03:48.933\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_file\u001b[0m:\u001b[36m86\u001b[0m - \u001b[34m\u001b[1mAnalyze time: 0.000251s\u001b[0m\n",
      "\u001b[32m2025-04-29 14:03:48.933\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_file\u001b[0m:\u001b[36m96\u001b[0m - \u001b[1mTotal time: 0.015285s\u001b[0m\n",
      "\u001b[32m2025-04-29 14:03:48.937\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_file\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mLoading './data/raw/geonames/timeZones.txt'...\u001b[0m\n",
      "\u001b[32m2025-04-29 14:03:48.938\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_file\u001b[0m:\u001b[36m51\u001b[0m - \u001b[34m\u001b[1mScan time: 0.000189s\u001b[0m\n",
      "\u001b[32m2025-04-29 14:03:48.939\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_file\u001b[0m:\u001b[36m58\u001b[0m - \u001b[34m\u001b[1mCollect time: 0.001051s\u001b[0m\n",
      "\u001b[32m2025-04-29 14:03:48.940\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_file\u001b[0m:\u001b[36m65\u001b[0m - \u001b[34m\u001b[1mWrite time: 0.001062s\u001b[0m\n",
      "\u001b[32m2025-04-29 14:03:48.943\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_file\u001b[0m:\u001b[36m79\u001b[0m - \u001b[34m\u001b[1mCreate time: 0.002270s\u001b[0m\n",
      "\u001b[32m2025-04-29 14:03:48.944\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_file\u001b[0m:\u001b[36m83\u001b[0m - \u001b[34m\u001b[1mCommit time: 0.000555s\u001b[0m\n",
      "\u001b[32m2025-04-29 14:03:48.944\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_file\u001b[0m:\u001b[36m86\u001b[0m - \u001b[34m\u001b[1mAnalyze time: 0.000151s\u001b[0m\n",
      "\u001b[32m2025-04-29 14:03:48.944\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_file\u001b[0m:\u001b[36m96\u001b[0m - \u001b[1mTotal time: 0.007290s\u001b[0m\n",
      "\u001b[32m2025-04-29 14:03:49.456\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m331\u001b[0m - \u001b[34m\u001b[1mTable 'shapes' created\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<duckdb.duckdb.DuckDBPyConnection at 0x109029d70>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GID = \"geonameId\"\n",
    "\n",
    "\n",
    "def table_exists(con: DuckDBPyConnection, table_name: str) -> bool:\n",
    "    return table_name in con.execute(\"SHOW TABLES\").pl()[\"name\"]\n",
    "\n",
    "\n",
    "# Read and load 'allCountries.txt'\n",
    "# Function to read and load other files with different schemas\n",
    "def load_file(\n",
    "    # con: DuckDBPyConnection,\n",
    "    file_path: str,\n",
    "    schema: dict[str, Type[pl.DataType]],\n",
    "    table_name: str,\n",
    "    table_definition: str | None = None,\n",
    "    pipe: Callable[[pl.LazyFrame], pl.LazyFrame] | None = None,\n",
    "    has_header: bool = False,\n",
    "    skip_rows: int = 0,\n",
    "    overwrite: bool = False,\n",
    "    extra_expr: pl.Expr | None = None,\n",
    "):\n",
    "    if table_exists(con, table_name):\n",
    "        logger.debug(f\"Table '{table_name}' already exists\")\n",
    "        if not overwrite:\n",
    "            return\n",
    "        logger.debug(f\"Overwriting table '{table_name}'\")\n",
    "        con.execute(f\"DROP TABLE {table_name} CASCADE\")\n",
    "        logger.debug(f\"Table '{table_name}' dropped\")\n",
    "    time_start = time()\n",
    "    load = con.begin()\n",
    "    try:\n",
    "        logger.info(f\"Loading '{file_path}'...\")\n",
    "        # Time scan\n",
    "        time_scan = time()\n",
    "        q = pl.scan_csv(\n",
    "            file_path,\n",
    "            separator=\"\\t\",\n",
    "            has_header=has_header,\n",
    "            schema=schema,\n",
    "            skip_rows=skip_rows,\n",
    "        )\n",
    "        q = q.with_columns(\n",
    "            pl.col(pl.Utf8).str.strip_chars().str.strip_chars(\"\\\"':\").str.strip_chars()\n",
    "        )\n",
    "        if extra_expr is not None:\n",
    "            q = q.with_columns(extra_expr)\n",
    "        if pipe is not None:\n",
    "            q = q.pipe(pipe)\n",
    "        if GID in schema:\n",
    "            q = q.sort(GID, nulls_last=True)\n",
    "        logger.debug(f\"Scan time: {time() - time_scan:.6f}s\")\n",
    "\n",
    "        q = q.with_columns(cs.by_dtype(pl.String).str.strip_chars().replace(\"\", None))\n",
    "\n",
    "        # Time collect\n",
    "        time_collect = time()\n",
    "        df = q.collect()\n",
    "        logger.debug(f\"Collect time: {time() - time_collect:.6f}s\")\n",
    "\n",
    "        # Time write\n",
    "        time_write = time()\n",
    "        save_path = Path(f\"./data/processed/geonames/{table_name}.parquet\")\n",
    "        save_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        df.write_parquet(save_path.as_posix())\n",
    "        logger.debug(f\"Write time: {time() - time_write:.6f}s\")\n",
    "\n",
    "        # Time create\n",
    "        time_create = time()\n",
    "        # Create table with predefined schema if provided\n",
    "        time_create = time()\n",
    "        if table_definition:\n",
    "            # Create the table with specified schema\n",
    "            load.execute(table_definition)\n",
    "            load.from_arrow(df.to_arrow()).insert_into(table_name)\n",
    "        else:\n",
    "            # Use automatic schema derivation (your current approach)\n",
    "            load.from_arrow(df.to_arrow()).create(table_name)\n",
    "\n",
    "        logger.debug(f\"Create time: {time() - time_create:.6f}s\")\n",
    "\n",
    "        time_commit = time()\n",
    "        load.commit()\n",
    "        logger.debug(f\"Commit time: {time() - time_commit:.6f}s\")\n",
    "        analyze_time = time()\n",
    "        con.execute(\"VACUUM ANALYZE;\")\n",
    "        logger.debug(f\"Analyze time: {time() - analyze_time:.6f}s\")\n",
    "    except Exception as e:\n",
    "        logger.exception(\"Error loading '{file_path}'\")\n",
    "        logger.debug(e.with_traceback(None))\n",
    "        # Time rollback\n",
    "        time_rollback = time()\n",
    "        load.rollback()\n",
    "        logger.warning(f\"Rollback time: {time() - time_rollback:.6f}s\")\n",
    "        raise e\n",
    "    finally:\n",
    "        logger.info(f\"Total time: {time() - time_start:.6f}s\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def drop_duplicates(df: pl.LazyFrame) -> pl.LazyFrame:\n",
    "    cols = [\n",
    "        \"name\",\n",
    "        \"asciiname\",\n",
    "        \"feature_class\",\n",
    "        \"feature_code\",\n",
    "        \"admin0_code\",\n",
    "        \"admin1_code\",\n",
    "        \"admin2_code\",\n",
    "        \"admin3_code\",\n",
    "        \"admin4_code\",\n",
    "        \"timezone\",\n",
    "    ]\n",
    "    return (\n",
    "        df.sort(\"modification_date\", descending=True)\n",
    "        .unique(cols, keep=\"first\")\n",
    "        .filter(~pl.all_horizontal(pl.col(cols).is_null()))\n",
    "        .sort(\"geonameId\")\n",
    "    )\n",
    "\n",
    "\n",
    "schema_all_countries = {\n",
    "    GID: pl.UInt32,\n",
    "    \"name\": pl.Utf8,\n",
    "    \"asciiname\": pl.Utf8,\n",
    "    \"alternatenames\": pl.Utf8,\n",
    "    \"latitude\": pl.Float32,\n",
    "    \"longitude\": pl.Float32,\n",
    "    \"feature_class\": pl.Categorical,\n",
    "    \"feature_code\": pl.Categorical,\n",
    "    \"admin0_code\": pl.Categorical,\n",
    "    \"cc2\": pl.Utf8,\n",
    "    \"admin1_code\": pl.Utf8,\n",
    "    \"admin2_code\": pl.Utf8,\n",
    "    \"admin3_code\": pl.Utf8,\n",
    "    \"admin4_code\": pl.Utf8,\n",
    "    \"population\": pl.Int64,\n",
    "    \"elevation\": pl.Int32,\n",
    "    \"dem\": pl.Int32,\n",
    "    \"timezone\": pl.Categorical,\n",
    "    \"modification_date\": pl.Date,\n",
    "}\n",
    "\n",
    "\n",
    "load_file(\n",
    "    \"./data/raw/geonames/allCountries.txt\",\n",
    "    schema_all_countries,\n",
    "    \"allCountries\",\n",
    "    table_definition=sql_file(\"create_table_allCountries.sql\"),\n",
    "    pipe=drop_duplicates,\n",
    ")\n",
    "\n",
    "\n",
    "load_file(\n",
    "    \"./data/raw/geonames/allCountriesPostCode.txt\",\n",
    "    {\n",
    "        \"admin_code0\": pl.Categorical,\n",
    "        \"postal_code\": pl.Utf8,\n",
    "        \"place_name\": pl.Utf8,\n",
    "        \"admin_name1\": pl.Utf8,\n",
    "        \"admin_code1\": pl.Utf8,\n",
    "        \"admin_name2\": pl.Utf8,\n",
    "        \"admin_code2\": pl.Utf8,\n",
    "        \"admin_name3\": pl.Utf8,\n",
    "        \"admin_code3\": pl.Utf8,\n",
    "        \"latitude\": pl.Float32,\n",
    "        \"longitude\": pl.Float32,\n",
    "        \"accuracy\": pl.Int32,\n",
    "    },\n",
    "    \"allPostCodes\",\n",
    ")\n",
    "\n",
    "\n",
    "# Load other files with respective schemas\n",
    "load_file(\n",
    "    \"./data/raw/geonames/admin1CodesASCII.txt\",\n",
    "    {\n",
    "        \"code\": pl.Utf8,\n",
    "        \"name\": pl.Utf8,\n",
    "        \"name_ascii\": pl.Utf8,\n",
    "        GID: pl.UInt32,\n",
    "    },\n",
    "    \"admin1CodesASCII\",\n",
    "    table_definition=sql_file(\"create_table_admin1CodesASCII.sql\"),\n",
    ")\n",
    "\n",
    "load_file(\n",
    "    \"./data/raw/geonames/admin2Codes.txt\",\n",
    "    {\n",
    "        \"code\": pl.Utf8,\n",
    "        \"name\": pl.Utf8,\n",
    "        \"asciiname\": pl.Utf8,\n",
    "        GID: pl.UInt32,\n",
    "    },\n",
    "    \"admin2Codes\",\n",
    "    table_definition=sql_file(\"create_table_admin2Codes.sql\"),\n",
    ")\n",
    "\n",
    "\n",
    "def drop_invalid_gids(df: pl.LazyFrame, con: DuckDBPyConnection) -> pl.LazyFrame:\n",
    "    ids = con.execute(\"SELECT geonameId FROM allCountries\").pl().unique().to_series()\n",
    "    return df.filter(pl.col(GID).is_in(ids))\n",
    "\n",
    "\n",
    "load_file(\n",
    "    \"./data/raw/geonames/adminCode5.txt\",\n",
    "    {\n",
    "        GID: pl.UInt32,\n",
    "        \"adm5code\": pl.Utf8,\n",
    "    },\n",
    "    \"adminCode5\",\n",
    "    table_definition=sql_file(\"create_table_adminCode5.sql\"),\n",
    "    pipe=partial(drop_invalid_gids, con=con),\n",
    ")\n",
    "\n",
    "\n",
    "load_file(\n",
    "    \"./data/raw/geonames/alternateNamesV2.txt\",\n",
    "    {\n",
    "        \"alternateNameId\": pl.Int32,\n",
    "        GID: pl.UInt32,\n",
    "        \"isolanguage\": pl.Utf8,\n",
    "        \"alternate_name\": pl.Utf8,\n",
    "        \"isPreferredName\": pl.Int8,\n",
    "        \"isShortName\": pl.Int8,\n",
    "        \"isColloquial\": pl.Int8,\n",
    "        \"isHistoric\": pl.Int8,\n",
    "        \"from\": pl.Utf8,\n",
    "        \"to\": pl.Utf8,\n",
    "    },\n",
    "    \"alternateNamesV2\",\n",
    "    table_definition=sql_file(\"create_table_alternateNamesV2.sql\"),\n",
    "    extra_expr=cs.by_dtype(pl.Int8).cast(pl.Boolean).fill_null(False),\n",
    "    pipe=partial(drop_invalid_gids, con=con),\n",
    ")\n",
    "\n",
    "load_file(\n",
    "    \"./data/raw/geonames/countryInfo.txt\",\n",
    "    {\n",
    "        \"ISO\": pl.Categorical,\n",
    "        \"ISO3\": pl.Categorical,\n",
    "        \"ISO_Numeric\": pl.Int32,\n",
    "        \"fips\": pl.Categorical,\n",
    "        \"Country\": pl.Utf8,\n",
    "        \"Capital\": pl.Utf8,\n",
    "        \"Area\": pl.Float32,\n",
    "        \"Population\": pl.Int32,\n",
    "        \"Continent\": pl.Categorical,\n",
    "        \"tld\": pl.Utf8,\n",
    "        \"CurrencyCode\": pl.Utf8,\n",
    "        \"CurrencyName\": pl.Utf8,\n",
    "        \"Phone\": pl.Utf8,\n",
    "        \"Postal_Code_Format\": pl.Utf8,\n",
    "        \"Postal_Code_Regex\": pl.Utf8,\n",
    "        \"Languages\": pl.Utf8,\n",
    "        GID: pl.UInt32,\n",
    "        \"neighbours\": pl.Utf8,\n",
    "        \"EquivalentFipsCode\": pl.Utf8,\n",
    "    },\n",
    "    \"countryInfo\",\n",
    "    table_definition=sql_file(\"create_table_countryInfo.sql\"),\n",
    "    skip_rows=51,\n",
    ")\n",
    "\n",
    "load_file(\n",
    "    \"./data/raw/geonames/featureCodes_en.txt\",\n",
    "    {\n",
    "        \"code\": pl.Categorical,\n",
    "        \"name\": pl.Utf8,\n",
    "        \"description\": pl.Utf8,\n",
    "    },\n",
    "    \"featureCodes\",\n",
    "    table_definition=sql_file(\"create_table_featureCodes.sql\"),\n",
    ")\n",
    "\n",
    "\n",
    "def remove_old_ids(df: pl.LazyFrame, con: DuckDBPyConnection) -> pl.LazyFrame:\n",
    "    ids = con.execute(\"SELECT geonameId FROM allCountries\").pl().unique().to_series()\n",
    "    return (\n",
    "        df.filter(pl.col(\"parentId\").is_in(ids) & pl.col(\"childId\").is_in(ids))\n",
    "        .with_columns(\n",
    "            pl.when(pl.col(\"type\").str.contains(\"adm\", literal=True))\n",
    "            .then(pl.col(\"type\").str.to_uppercase())\n",
    "            .otherwise(pl.col(\"type\"))\n",
    "        )\n",
    "        .unique([\"parentId\", \"childId\"])\n",
    "    )\n",
    "\n",
    "\n",
    "load_file(\n",
    "    \"./data/raw/geonames/hierarchy.txt\",\n",
    "    {\n",
    "        \"parentId\": pl.UInt32,\n",
    "        \"childId\": pl.UInt32,\n",
    "        \"type\": pl.Utf8,\n",
    "    },\n",
    "    \"hierarchy\",\n",
    "    table_definition=sql_file(\"create_table_hierarchy.sql\"),\n",
    "    pipe=partial(remove_old_ids, con=con),\n",
    ")\n",
    "con.execute(sql_file(\"create_table_unique_ids.sql\"))\n",
    "\n",
    "load_file(\n",
    "    \"./data/raw/geonames/iso-languagecodes.txt\",\n",
    "    {\n",
    "        \"ISO_639_3\": pl.Utf8,\n",
    "        \"ISO_639_2\": pl.Utf8,\n",
    "        \"ISO_639_1\": pl.Utf8,\n",
    "        \"Language_Name\": pl.Utf8,\n",
    "    },\n",
    "    \"iso_languagecodes\",\n",
    "    table_definition=sql_file(\"create_table_iso_languagecodes.sql\"),\n",
    ")\n",
    "\n",
    "\n",
    "load_file(\n",
    "    \"./data/raw/geonames/timeZones.txt\",\n",
    "    {\n",
    "        \"CountryCode\": pl.Utf8,\n",
    "        \"TimeZoneId\": pl.Utf8,\n",
    "        \"GMT_offset_1_Jan_2024\": pl.Float32,\n",
    "        \"DST_offset_1_Jul_2024\": pl.Float32,\n",
    "        \"rawOffset\": pl.Float32,\n",
    "    },\n",
    "    \"timeZones\",\n",
    "    table_definition=sql_file(\"create_table_timeZones.sql\"),\n",
    "    skip_rows=1,\n",
    ")\n",
    "# # Ignore loading the geo data for now\n",
    "if not table_exists(con, \"shapes\"):\n",
    "    con.execute(sql_file(\"create_table_shapes.sql\"))\n",
    "    logger.debug(\"Table 'shapes' created\")\n",
    "\n",
    "# # File is corupted atm\n",
    "# load_file(\n",
    "#     \"./data/raw/geonames/userTags.txt\",\n",
    "#     {\n",
    "#         GID: pl.Int32,\n",
    "#         \"tag\": pl.Utf8,\n",
    "#     },\n",
    "#     \"userTags\",\n",
    "# )\n",
    "con.execute(sql_file(\"create_table_equivalent.sql\"))\n",
    "con.execute(sql_file(\"create_view_cities.sql\"))\n",
    "con.execute(sql_file(\"create_view_locations_full.sql\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create country table\n",
    "# con.execute(sql_file(\"create_table_equivalent.sql\")).pl()\n",
    "# con.execute(sql_file(\"create_table_admin0.sql\")).execute(\"\"\"PRAGMA create_fts_index(\n",
    "#     admin0,\n",
    "#     geonameId,\n",
    "#     name,\n",
    "#     asciiname,\n",
    "#     official_name,\n",
    "#     alternatenames,\n",
    "#     admin0_code,\n",
    "#     ISO3,\n",
    "#     ISO_Numeric,\n",
    "#     fips,\n",
    "#     stemmer = 'none',\n",
    "#     stopwords = 'none',\n",
    "#     ignore = '(\\\\.|[^a-z0-9])+',\n",
    "#     overwrite = 1\n",
    "# );\"\"\")\n",
    "\n",
    "# con.execute(sql_file(\"create_view_*_NODES.sql\", table=\"admin0\"))\n",
    "\n",
    "# con.execute(sql_file(\"create_view_*_FTS.sql\", table=\"admin0\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%skip` not found.\n"
     ]
    }
   ],
   "source": [
    "%skip\n",
    "# if (path := Path(\"./data/processed/latlon.index\")).exists():\n",
    "#     logger.debug(\"Loading index...\")\n",
    "#     index = Index.restore(path, view=True) or raise ValueError(\"Failed to load index\")\n",
    "# else:\n",
    "#     logger.debug(\"Creating index...\")\n",
    "#     coordinates = df.select([\"latitude\", \"longitude\"]).to_numpy(order=\"c\")\n",
    "#     labels = df[\"geonameid\"].to_numpy()\n",
    "#     index: Index = Index(ndim=2, metric=\"haversine\", dtype=\"f32\")\n",
    "#     index.add(keys=labels, vectors=coordinates, log=True)\n",
    "#     index.save(path)\n",
    "\n",
    "\n",
    "class VectorIndex:\n",
    "    default_index_path = Path(\"./data/indexes/vector\")\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        index_name: str,\n",
    "        data: pl.DataFrame | None = None,\n",
    "        id_column: str = \"geonameId\",\n",
    "        main_column: str = \"vectors\",\n",
    "        metric: str = \"L2\",\n",
    "        embedder: SentenceTransformer | None = None,\n",
    "    ):\n",
    "        self._index_path = self.default_index_path / f\"{index_name}.index\"\n",
    "        self._id_column = id_column\n",
    "        self._main_column = main_column\n",
    "        self._metric = metric\n",
    "        index = self.get_or_build_index(data, metric)\n",
    "        if isinstance(index, Err):\n",
    "            logger.debug(\n",
    "                f\"Index does not exist at '{self.index_path}', build index with 'build_index' method.\"\n",
    "            )\n",
    "            self._index = None  # type: ignore\n",
    "        else:\n",
    "            self._index: Index = index.ok_value\n",
    "\n",
    "    @property\n",
    "    def index(self) -> Index:\n",
    "        return self._index\n",
    "\n",
    "    @property\n",
    "    def id_column(self) -> str:\n",
    "        return self._id_column\n",
    "\n",
    "    @property\n",
    "    def main_column(self) -> str:\n",
    "        return self._main_column\n",
    "\n",
    "    @property\n",
    "    def index_path(self) -> Path:\n",
    "        return self._index_path\n",
    "\n",
    "    @property\n",
    "    def ndims(self) -> int:\n",
    "        return self._ndims\n",
    "\n",
    "    @property\n",
    "    def metric(self) -> str:\n",
    "        return self._metric\n",
    "\n",
    "    def _build_index(\n",
    "        self,\n",
    "        df: pl.DataFrame,\n",
    "        metric: str = \"L2\",  # TODO: Metric like\n",
    "    ) -> Result[Index, str]:\n",
    "        \"\"\"Data passed should be an Id and a vector.\"\"\"\n",
    "        logger.debug(\"Creating index...\")\n",
    "        vectors = df[self.main_column].to_numpy()\n",
    "        labels = df[self.id_column].to_numpy()\n",
    "        ndims = vectors.shape[1]  # Find n dims\n",
    "        index: Index = Index(ndim=ndims, metric=metric, dtype=\"f32\")\n",
    "        index.add(keys=labels, vectors=vectors, log=True)\n",
    "        index.save(self.index_path)\n",
    "        return Ok(index)\n",
    "\n",
    "    def get_index(self) -> Result[Index, str]:\n",
    "        if (path := self.index_path).exists():\n",
    "            logger.debug(f\"Opening index at '{self.index_path}'\")\n",
    "            index = Index.restore(path, view=True)\n",
    "            if index is not None:\n",
    "                return Ok(index)\n",
    "        return Err(f\"Index does not exist at '{self.index_path}'\")\n",
    "\n",
    "    def get_or_build_index(\n",
    "        self,\n",
    "        df: pl.DataFrame | None = None,\n",
    "        metric: str = \"L2\",  # TODO: as above\n",
    "    ) -> Result[Index, str]:\n",
    "        self.index_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        if not self.index_path.exists():\n",
    "            if df is None:\n",
    "                return Err(\n",
    "                    \"Index does not exist. DataFrame is required to create index\"\n",
    "                )\n",
    "            match self._build_index(df, metric):\n",
    "                case Ok(index):\n",
    "                    ...\n",
    "                case Err(e):\n",
    "                    return Err(e)\n",
    "        else:\n",
    "            match self.get_index():\n",
    "                case Ok(index):\n",
    "                    ...\n",
    "                case Err(e):\n",
    "                    return Err(e)\n",
    "\n",
    "        self._ndims = index.ndim\n",
    "        logger.debug(\"Opening index\")\n",
    "        return Ok(index)\n",
    "\n",
    "    def search(\n",
    "        self,\n",
    "        query: NDArray[np.float32],\n",
    "        limit: int = 10,\n",
    "        include: list[int] | None = None,\n",
    "        exclude: list[int] | None = None,\n",
    "    ) -> Result[pl.DataFrame, str]:\n",
    "        return self.vector_search(query, limit, include, exclude)\n",
    "\n",
    "    def vector_search(\n",
    "        self,\n",
    "        query: NDArray[np.float32],\n",
    "        limit: int = 10,\n",
    "        include: list[int] | None = None,\n",
    "        exclude: list[int] | None = None,\n",
    "        exact: bool = False,\n",
    "    ) -> Result[pl.DataFrame, str]:\n",
    "        output = self.index.search(vectors=query, count=limit, log=True, exact=exact)\n",
    "\n",
    "        logger.debug(f\"Visited members: {output.visited_members}\")\n",
    "        logger.debug(f\"Computed distances: {output.computed_distances}\")\n",
    "\n",
    "        # Extract keys (geonameids) and distances\n",
    "        keys = output.keys\n",
    "        distances = output.distances\n",
    "\n",
    "        # Create a DataFrame from the search results\n",
    "        results_df = pl.LazyFrame(\n",
    "            data={self.id_column: keys, \"score\": distances},\n",
    "            schema={self.id_column: pl.UInt32, \"score\": pl.Float32},\n",
    "        )\n",
    "        if self.metric == \"haversine\":\n",
    "            results_df = results_df.with_columns(pl.col(\"score\") * 6371.0)\n",
    "\n",
    "        results_df = results_df.sort(\n",
    "            \"score\"\n",
    "        )  # TODO: ascending descending depending on metric.\n",
    "\n",
    "        return Ok(results_df.collect())\n",
    "\n",
    "\n",
    "class FTSIndex:\n",
    "    default_index_path = Path(\"./data/indexes/fts\")\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        index_name: str,\n",
    "        data: pl.DataFrame | None = None,\n",
    "        id_column: str = \"geonameId\",\n",
    "        main_column: str = \"name\",\n",
    "    ):\n",
    "        self._index_path = self.default_index_path / index_name\n",
    "        self._column_types = {}\n",
    "        self._id_column = id_column\n",
    "        self._main_column = main_column\n",
    "        index = self.get_or_build_index(data)\n",
    "        if isinstance(index, Err):\n",
    "            logger.debug(\n",
    "                f\"Index does not exist at '{self.index_path}', build index with 'build_index' method.\"\n",
    "            )\n",
    "            self._index = None  # type: ignore\n",
    "        else:\n",
    "            self._index: tantivy.Index = index.ok_value\n",
    "\n",
    "    @property\n",
    "    def index(self) -> tantivy.Index:\n",
    "        self._index.reload()\n",
    "        return self._index\n",
    "\n",
    "    @property\n",
    "    def column_types(self) -> dict[str, str]:\n",
    "        return self._column_types\n",
    "\n",
    "    @property\n",
    "    def id_column(self) -> str:\n",
    "        return self._id_column\n",
    "\n",
    "    @property\n",
    "    def main_column(self) -> str:\n",
    "        return self._main_column\n",
    "\n",
    "    @property\n",
    "    def index_path(self) -> Path:\n",
    "        return self._index_path\n",
    "\n",
    "    @property\n",
    "    def columns_not_id(self) -> list[str]:\n",
    "        return [col for col in self.column_types if col != self.id_column]\n",
    "\n",
    "    def _build_index(\n",
    "        self,\n",
    "        df: pl.DataFrame,\n",
    "        split_field: dict[str, list[str] | str] | None = None,\n",
    "    ) -> Result[tantivy.Index, str]:\n",
    "        \"\"\"Only pass in data which you wish to build the ftx index with. split_field is a dictionary of fields to split by a delimiter. eg {\",\": [\"field1\", \"field2\"]} will split field1 and field2 by comma.\"\"\"\n",
    "        # TODO: this programmatically into tantivy schema\n",
    "        schema_builder = tantivy.SchemaBuilder()\n",
    "\n",
    "        if self.id_column not in df.columns:\n",
    "            return Err(f\"'{self.id_column}' column not found in DataFrame\")\n",
    "\n",
    "        col_types = {}\n",
    "        for col in df.columns:\n",
    "            if col == self.id_column:\n",
    "                schema_builder.add_integer_field(\n",
    "                    self.id_column, stored=True, indexed=True, fast=True\n",
    "                )\n",
    "            # TODO: ADD support for other types\n",
    "            else:\n",
    "                schema_builder.add_text_field(col)\n",
    "            col_types[col] = df[col].dtype._string_repr()\n",
    "\n",
    "        self._column_types = col_types\n",
    "\n",
    "        schema = schema_builder.build()\n",
    "        logger.debug(f\"Creating index with columns:\\n{json.dumps(col_types, indent=2)}\")\n",
    "\n",
    "        index = tantivy.Index(schema, path=self.index_path.as_posix(), reuse=False)\n",
    "        writer = index.writer()\n",
    "        for row in df.rows(named=True):\n",
    "            if split_field:\n",
    "                for splitter, fields in split_field.items():\n",
    "                    if isinstance(fields, str):\n",
    "                        fields = [fields]\n",
    "                    for field in fields:\n",
    "                        logger.debug(f\"Splitting {field} by {splitter}...\")\n",
    "                        row[field] = row[field].split(splitter)\n",
    "            writer.add_document(tantivy.Document(**row))\n",
    "        writer.commit()\n",
    "        writer.wait_merging_threads()\n",
    "        return Ok(index)\n",
    "\n",
    "    def get_index(self) -> Result[tantivy.Index, str]:\n",
    "        if tantivy.Index.exists(self.index_path.as_posix()):\n",
    "            logger.debug(f\"Opening index at '{self.index_path}'\")\n",
    "            return Ok(tantivy.Index.open(self.index_path.as_posix()))\n",
    "        return Err(f\"Index does not exist at '{self.index_path}'\")\n",
    "\n",
    "    def get_or_build_index(\n",
    "        self, df: pl.DataFrame | None = None\n",
    "    ) -> Result[tantivy.Index, str]:\n",
    "        if not self.index_path.exists() and df is None:\n",
    "            return Err(\"Index does not exist. DataFrame is required to create index\")\n",
    "\n",
    "        self.index_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        if not tantivy.Index.exists(self.index_path.as_posix()):\n",
    "            if df is None:\n",
    "                return Err(\"DataFrame is required to create index\")\n",
    "            match self._build_index(df):\n",
    "                case Ok(index):\n",
    "                    ...\n",
    "                case Err(e):\n",
    "                    return Err(e)\n",
    "        else:\n",
    "            match self.get_index():\n",
    "                case Ok(index):\n",
    "                    ...\n",
    "                case Err(e):\n",
    "                    return Err(e)\n",
    "        schema = json.loads((self.index_path / \"meta.json\").read_text())[\"schema\"]\n",
    "        sc = {}\n",
    "        for v in schema:\n",
    "            type_ = v[\"type\"]\n",
    "            if type_ == \"text\":\n",
    "                type_ = pl.Utf8\n",
    "            elif type_ == \"i64\":\n",
    "                type_ = pl.UInt32\n",
    "            sc[v[\"name\"]] = type_\n",
    "\n",
    "        self._column_types = sc\n",
    "        logger.debug(\"Schema Loaded\")\n",
    "        logger.debug(\"Opening country index\")\n",
    "        return Ok(index)\n",
    "\n",
    "    def convert_fts_results(\n",
    "        self, hits: tantivy.SearchResult, searcher: tantivy.Searcher\n",
    "    ) -> pl.DataFrame:\n",
    "        logger.debug(f\"FTS hits from search: {hits.count}\")  # type: ignore\n",
    "\n",
    "        scores, gids = zip(\n",
    "            *[\n",
    "                (score, searcher.doc(doc).get_first(self.id_column))\n",
    "                for score, doc in hits.hits\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        return (\n",
    "            pl.LazyFrame(\n",
    "                {\"geonameId\": list(gids), \"score\": list(scores)},\n",
    "                schema={\"geonameId\": pl.UInt32, \"score\": pl.Float32},\n",
    "            )\n",
    "            .sort(\"score\", descending=True, maintain_order=True)\n",
    "            .collect()\n",
    "        )\n",
    "\n",
    "    def search(\n",
    "        self,\n",
    "        query: str,\n",
    "        limit: int = 10,\n",
    "        include: list[int] | None = None,\n",
    "        exclude: list[int] | None = None,\n",
    "    ) -> Result[pl.DataFrame, str]:\n",
    "        return self.fts_search(\n",
    "            query,\n",
    "            limit=limit,\n",
    "            include=include,\n",
    "            exclude=exclude,\n",
    "        )\n",
    "\n",
    "    def fts_search(\n",
    "        self,\n",
    "        query: str,\n",
    "        limit: int = 10,\n",
    "        include: list[int] | None = None,\n",
    "        exclude: list[int] | None = None,\n",
    "        main_term_query_boost: float = 3.0,\n",
    "        fuzzy_term_query_boost: float = 2.0,\n",
    "        max_fuzzy_distance: int = 2,\n",
    "        phrase: bool = True,\n",
    "    ) -> Result[pl.DataFrame, str]:\n",
    "        # Create for list of queries (batch search)\n",
    "        if phrase:\n",
    "            query = f\"'{query}'\"\n",
    "        else:\n",
    "            query = query.strip(\"\\\"'\")\n",
    "        query = query.strip()\n",
    "        index = self.index\n",
    "\n",
    "        searcher = index.searcher()\n",
    "\n",
    "        bool_query_list: list[tuple[tantivy.Occur, tantivy.Query]] = []\n",
    "\n",
    "        # Calculate fuzzy distance based on query length\n",
    "        fuzzy_distance = min(max(0, len(query) - 2), max_fuzzy_distance)\n",
    "\n",
    "        if self.main_column in self.columns_not_id:\n",
    "            main_term_query = tantivy.Query.term_query(\n",
    "                index.schema, self.main_column, query\n",
    "            )\n",
    "            bool_query_list.append(\n",
    "                (\n",
    "                    tantivy.Occur.Should,\n",
    "                    tantivy.Query.boost_query(main_term_query, main_term_query_boost),\n",
    "                )\n",
    "            )\n",
    "\n",
    "            if fuzzy_distance > 0:\n",
    "                main_fuzzy_query = tantivy.Query.fuzzy_term_query(\n",
    "                    index.schema, self.main_column, query, distance=fuzzy_distance\n",
    "                )\n",
    "\n",
    "                bool_query_list.append(\n",
    "                    (\n",
    "                        tantivy.Occur.Should,\n",
    "                        tantivy.Query.boost_query(\n",
    "                            main_fuzzy_query, fuzzy_term_query_boost\n",
    "                        ),\n",
    "                    )\n",
    "                )\n",
    "\n",
    "            rest_of_query = index.parse_query(\n",
    "                query, list(set(self.columns_not_id) - {self.main_column})\n",
    "            )\n",
    "            bool_query_list.append((tantivy.Occur.Should, rest_of_query))\n",
    "\n",
    "        if include:\n",
    "            bool_query_list.append(\n",
    "                (\n",
    "                    tantivy.Occur.Must,\n",
    "                    tantivy.Query.term_set_query(index.schema, self.id_column, include),\n",
    "                )\n",
    "            )\n",
    "        if exclude:\n",
    "            bool_query_list.append(\n",
    "                (\n",
    "                    tantivy.Occur.MustNot,\n",
    "                    tantivy.Query.term_set_query(index.schema, self.id_column, exclude),\n",
    "                )\n",
    "            )\n",
    "        if bool_query_list:\n",
    "            final_query = tantivy.Query.boolean_query(bool_query_list)\n",
    "\n",
    "        else:\n",
    "            final_query: tantivy.Query = index.parse_query(\n",
    "                query, default_field_names=self.columns_not_id\n",
    "            )\n",
    "\n",
    "        logger.debug(final_query)\n",
    "\n",
    "        hits: tantivy.SearchResult = searcher.search(final_query, limit=limit)\n",
    "\n",
    "        if hits.count == 0:  # type: ignore\n",
    "            if phrase:\n",
    "                logger.debug(\"No results found, retrying without phrase search...\")\n",
    "                return self.fts_search(\n",
    "                    query,\n",
    "                    limit,\n",
    "                    include,\n",
    "                    exclude,\n",
    "                    main_term_query_boost,\n",
    "                    fuzzy_term_query_boost,\n",
    "                    max_fuzzy_distance,\n",
    "                    phrase=False,\n",
    "                )\n",
    "            return Err(\"No results found\")\n",
    "\n",
    "        return Ok(self.convert_fts_results(hits, searcher))\n",
    "\n",
    "\n",
    "class HybridIndex:\n",
    "    def __init__(self, fts_idx: FTSIndex, vidx: VectorIndex):\n",
    "        self._fts_idx = fts_idx\n",
    "        self._vidx = vidx\n",
    "\n",
    "    @property\n",
    "    def vector_index(self) -> VectorIndex:\n",
    "        return self._vidx\n",
    "\n",
    "    @property\n",
    "    def fts_index(self) -> FTSIndex:\n",
    "        return self._fts_idx\n",
    "\n",
    "    def search(\n",
    "        self,\n",
    "        query: str,\n",
    "        limit: int = 10,\n",
    "        include: list[int] | None = None,\n",
    "        exclude: list[int] | None = None,\n",
    "        main_term_query_boost: float = 3.0,\n",
    "        fuzzy_term_query_boost: float = 2.0,\n",
    "        max_fuzzy_distance: int = 2,\n",
    "        phrase: bool = True,\n",
    "    ) -> Result[pl.DataFrame, str]:\n",
    "        v_search = self.vector_index.vector_search\n",
    "\n",
    "\n",
    "country_index = FTSIndex(\"admin0\", con.table(\"admin0_FTS\").pl())\n",
    "country_index.fts_search(\"An Danmhairg\").unwrap().join(\n",
    "    con.table(\"admin0\").pl(), \"geonameId\", \"left\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-04-29 14:07:26.403\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m10\u001b[0m - \u001b[34m\u001b[1mLoaded 534106 entities and 508066 hierarchical relationships\u001b[0m\n",
      "\u001b[32m2025-04-29 14:07:26.403\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m13\u001b[0m - \u001b[34m\u001b[1mEntity columns:\u001b[0m\n",
      "\u001b[32m2025-04-29 14:07:26.404\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m14\u001b[0m - \u001b[34m\u001b[1mHierarchy columns:\u001b[0m\n",
      "\u001b[32m2025-04-29 14:07:26.434\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m27\u001b[0m - \u001b[34m\u001b[1mCreated Entity table\u001b[0m\n",
      "\u001b[32m2025-04-29 14:07:26.437\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m33\u001b[0m - \u001b[34m\u001b[1mCreated IsIn table\u001b[0m\n",
      "\u001b[32m2025-04-29 14:07:26.723\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m51\u001b[0m - \u001b[34m\u001b[1mLoaded Entity\u001b[0m\n",
      "\u001b[32m2025-04-29 14:07:26.843\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m57\u001b[0m - \u001b[34m\u001b[1mLoaded IsIn\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "entities_df = con.execute(f\"\"\"\n",
    "    SELECT {GID}, name, feature_class, feature_code\n",
    "    FROM unique_ids\n",
    "\"\"\").pl()\n",
    "\n",
    "hierarchy_df = con.execute(\"\"\"\n",
    "    SELECT parentId, childId, type\n",
    "    FROM hierarchy\n",
    "\"\"\").pl()\n",
    "logger.debug(\n",
    "    f\"Loaded {len(entities_df)} entities and {len(hierarchy_df)} hierarchical relationships\"\n",
    ")\n",
    "logger.debug(\"Entity columns:\", entities_df.columns)\n",
    "logger.debug(\"Hierarchy columns:\", hierarchy_df.columns)\n",
    "\n",
    "# 2. Setup Kuzu database connection\n",
    "gdb_path = Path(\"./data/db/graph_db\")\n",
    "gdb_path.mkdir(parents=True, exist_ok=True)\n",
    "gdb = kz.Database(gdb_path.as_posix())\n",
    "conn = kz.Connection(gdb)\n",
    "\n",
    "# 3. Create the schema in Kuzu if needed\n",
    "if \"Entity\" not in conn.execute(\"CALL SHOW_TABLES() RETURN *;\").get_as_pl().get_column(\n",
    "    \"name\"\n",
    "):\n",
    "    conn.execute(sql_file(\"create_node_entity.sql\"))\n",
    "    logger.debug(\"Created Entity table\")\n",
    "\n",
    "if \"IsIn\" not in conn.execute(\"CALL SHOW_TABLES() RETURN *;\").get_as_pl().get_column(\n",
    "    \"name\"\n",
    "):\n",
    "    conn.execute(sql_file(\"create_relation_IsIn.sql\"))\n",
    "    logger.debug(\"Created IsIn table\")\n",
    "\n",
    "    # 4. Check if tables already have data\n",
    "are_nodes = (\n",
    "    conn.execute(\"MATCH (e:Entity) RETURN count(e) > 0 AS HasData\")\n",
    "    .get_as_pl()\n",
    "    .get_column(\"HasData\")[0]\n",
    ")\n",
    "are_edges = (\n",
    "    conn.execute(\"MATCH ()-[r:IsIn]->() RETURN count(r) > 0 AS HasData\")\n",
    "    .get_as_pl()\n",
    "    .get_column(\"HasData\")[0]\n",
    ")\n",
    "\n",
    "if not are_nodes:\n",
    "    conn.execute(\n",
    "        f\"COPY Entity FROM (LOAD FROM entities_df RETURN {GID}, name, feature_class, feature_code)\"\n",
    "    )\n",
    "    logger.debug(\"Loaded Entity\")\n",
    "\n",
    "if not are_edges:\n",
    "    conn.execute(\n",
    "        \"COPY IsIn FROM (LOAD FROM hierarchy_df RETURN parentId, childId, type)\"\n",
    "    )\n",
    "    logger.debug(\"Loaded IsIn\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (4, 4)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>geonameId</th><th>name</th><th>feature_class</th><th>feature_code</th></tr><tr><td>i32</td><td>str</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>11820342</td><td>&quot;Horn of Africa&quot;</td><td>&quot;L&quot;</td><td>&quot;RGN&quot;</td></tr><tr><td>11812257</td><td>&quot;Commonwealth of Nations&quot;</td><td>&quot;A&quot;</td><td>&quot;ZN&quot;</td></tr><tr><td>7729889</td><td>&quot;Eastern Africa&quot;</td><td>&quot;L&quot;</td><td>&quot;RGN&quot;</td></tr><tr><td>6255146</td><td>&quot;Africa&quot;</td><td>&quot;L&quot;</td><td>&quot;CONT&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (4, 4)\n",
       "┌───────────┬─────────────────────────┬───────────────┬──────────────┐\n",
       "│ geonameId ┆ name                    ┆ feature_class ┆ feature_code │\n",
       "│ ---       ┆ ---                     ┆ ---           ┆ ---          │\n",
       "│ i32       ┆ str                     ┆ str           ┆ str          │\n",
       "╞═══════════╪═════════════════════════╪═══════════════╪══════════════╡\n",
       "│ 11820342  ┆ Horn of Africa          ┆ L             ┆ RGN          │\n",
       "│ 11812257  ┆ Commonwealth of Nations ┆ A             ┆ ZN           │\n",
       "│ 7729889   ┆ Eastern Africa          ┆ L             ┆ RGN          │\n",
       "│ 6255146   ┆ Africa                  ┆ L             ┆ CONT         │\n",
       "└───────────┴─────────────────────────┴───────────────┴──────────────┘"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_children_query(geoname_id: int) -> str:\n",
    "    query = f\"\"\"MATCH (p:Entity {{geonameId: {geoname_id}}})-[:IsIn]->(c:Entity)\n",
    "    RETURN c.{GID} AS {GID}, c.name AS name, c.feature_class AS feature_class, c.feature_code AS feature_code;\"\"\"\n",
    "    return query\n",
    "\n",
    "\n",
    "def get_parents_query(geoname_id):\n",
    "    query = f\"\"\"MATCH (c:Entity {{geonameId: {geoname_id}}})<-[:IsIn]-(p:Entity)\n",
    "    RETURN p.{GID} AS {GID}, p.name AS name, p.feature_class AS feature_class, p.feature_code AS feature_code;\"\"\"\n",
    "    return query\n",
    "\n",
    "\n",
    "# MATCH (c:Entity) WHERE CAST(c.geonameId, \"INT64\") IN list_creation({formatted_ids}) RETURN *;\n",
    "def get_children_querys(\n",
    "    geoname_ids: list[int] | pl.Series, traverse: bool = False\n",
    ") -> str:\n",
    "    query = f\"\"\"MATCH (p:Entity)-[:IsIn{\"*\" if traverse else \"\"}]->(c:Entity)\n",
    "    WHERE p.geonameId IN CAST({geoname_ids}, \"UINT32[]\")\n",
    "    RETURN DISTINCT c.{GID} AS {GID}, c.name AS name, c.feature_class AS feature_class, c.feature_code AS feature_code;\"\"\"\n",
    "    return query\n",
    "\n",
    "\n",
    "def get_parents_querys(\n",
    "    geoname_ids: list[int] | pl.Series, traverse: bool = False\n",
    ") -> str:\n",
    "    query = f\"\"\"MATCH (c:Entity)<-[:IsIn{\"*\" if traverse else \"\"}]-(p:Entity)\n",
    "    WHERE c.geonameId IN CAST({geoname_ids}, \"UINT32[]\")\n",
    "    RETURN DISTINCT p.{GID} AS {GID}, p.name AS name, p.feature_class AS feature_class, p.feature_code AS feature_code;\"\"\"\n",
    "    return query\n",
    "\n",
    "\n",
    "def get_highest_parent_query():\n",
    "    query = f\"\"\"\n",
    "    MATCH (entity:Entity)\n",
    "    WHERE NOT (entity)<-[:IsIn]-(:Entity)\n",
    "    RETURN entity.{GID} AS {GID}, entity.name AS name, entity.feature_class AS feature_class, entity.feature_code AS feature_code;\"\"\"\n",
    "    return query\n",
    "\n",
    "\n",
    "conn.execute(get_parents_query(49518)).get_as_pl()\n",
    "conn.execute(get_children_query(6252001)).get_as_pl()\n",
    "conn.execute(get_children_querys([49518, 51537])).get_as_pl()\n",
    "conn.execute(get_parents_querys([49518, 51537])).get_as_pl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_orphaned_admin_entities(con, conn):\n",
    "    \"\"\"Fix orphaned admin entities using both admin codes and graph traversal.\"\"\"\n",
    "\n",
    "    # Find entities with admin level feature codes that aren't in any admin table\n",
    "    orphans_query = \"\"\"\n",
    "    SELECT a.geonameId, a.name, a.feature_code, a.admin0_code, a.admin1_code, a.admin2_code, a.admin3_code\n",
    "    FROM allCountries a\n",
    "    WHERE a.feature_code LIKE 'ADM%'\n",
    "    AND a.geonameId NOT IN (\n",
    "        SELECT geonameId FROM admin0\n",
    "        UNION ALL SELECT geonameId FROM admin1\n",
    "        UNION ALL SELECT geonameId FROM admin2\n",
    "        UNION ALL SELECT geonameId FROM admin3\n",
    "        UNION ALL SELECT geonameId FROM admin4\n",
    "    )\n",
    "    \"\"\"\n",
    "\n",
    "    orphans_df = con.execute(orphans_query).pl()\n",
    "    if len(orphans_df) == 0:\n",
    "        logger.debug(\"No orphaned admin entities found!\")\n",
    "        return\n",
    "\n",
    "    logger.debug(f\"Found {len(orphans_df)} orphaned admin entities.\")\n",
    "\n",
    "    # Try to find parents using graph database first\n",
    "    orphan_ids = orphans_df[\"geonameId\"].to_list()\n",
    "    parents_query = f\"\"\"\n",
    "    MATCH (c:Entity)<-[:IsIn]-(p:Entity)\n",
    "    WHERE c.geonameId IN CAST({orphan_ids}, \"UINT32[]\")\n",
    "    RETURN c.geonameId AS geonameId, p.geonameId AS parent_id\n",
    "    \"\"\"\n",
    "\n",
    "    parent_links = conn.execute(parents_query).get_as_pl()\n",
    "\n",
    "    # For remaining orphans without graph parents, try to infer level and parent using admin codes\n",
    "    code_linked = 0\n",
    "    for orphan in orphans_df.filter(\n",
    "        ~pl.col(\"geonameId\").is_in(parent_links[\"geonameId\"])\n",
    "    ).iter_rows(named=True):\n",
    "        level = None\n",
    "        if \"ADM1\" in orphan[\"feature_code\"]:\n",
    "            level = 1\n",
    "        elif \"ADM2\" in orphan[\"feature_code\"]:\n",
    "            level = 2\n",
    "        elif \"ADM3\" in orphan[\"feature_code\"]:\n",
    "            level = 3\n",
    "        elif \"ADM4\" in orphan[\"feature_code\"]:\n",
    "            level = 4\n",
    "\n",
    "        if level:\n",
    "            # Based on level, try to add to appropriate admin table using code-based parent\n",
    "            con.execute(f\"\"\"\n",
    "            INSERT INTO admin{level} (\n",
    "                SELECT\n",
    "                    a.geonameId,\n",
    "                    a.name,\n",
    "                    a.asciiname,\n",
    "                    a.admin0_code,\n",
    "                    {\"a.admin1_code\" if level >= 1 else \"NULL AS admin1_code\"},\n",
    "                    {\"a.admin2_code\" if level >= 2 else \"NULL AS admin2_code\"},\n",
    "                    {\"a.admin3_code\" if level >= 3 else \"NULL AS admin3_code\"},\n",
    "                    {\"a.admin4_code\" if level >= 4 else \"NULL AS admin4_code\"},\n",
    "                    a.feature_class,\n",
    "                    a.feature_code,\n",
    "                    a.population,\n",
    "                    -- Try to find parent ID from previous level\n",
    "                    (\n",
    "                        SELECT parent.geonameId FROM admin{level - 1} parent\n",
    "                        WHERE parent.admin0_code = a.admin0_code\n",
    "                        {\"AND parent.admin1_code = a.admin1_code\" if level >= 2 else \"\"}\n",
    "                        {\"AND parent.admin2_code = a.admin2_code\" if level >= 3 else \"\"}\n",
    "                        {\"AND parent.admin3_code = a.admin3_code\" if level >= 4 else \"\"}\n",
    "                        LIMIT 1\n",
    "                    ) AS parent_id,\n",
    "                    c.name AS country_name,\n",
    "                    parent.name AS parent_name,\n",
    "                    a.alternatenames\n",
    "                FROM\n",
    "                    allCountries a\n",
    "                LEFT JOIN\n",
    "                    admin0 c ON a.admin0_code = c.admin0_code\n",
    "                LEFT JOIN\n",
    "                    admin{level - 1} parent ON\n",
    "                        parent.admin0_code = a.admin0_code\n",
    "                        {\"AND parent.admin1_code = a.admin1_code\" if level >= 2 else \"\"}\n",
    "                        {\"AND parent.admin2_code = a.admin2_code\" if level >= 3 else \"\"}\n",
    "                        {\"AND parent.admin3_code = a.admin3_code\" if level >= 4 else \"\"}\n",
    "                WHERE\n",
    "                    a.geonameId = {orphan[\"geonameId\"]}\n",
    "            )\n",
    "            \"\"\")\n",
    "            code_linked += 1\n",
    "\n",
    "    logger.debug(\n",
    "        f\"Fixed {len(parent_links)} orphans using graph relationships and {code_linked} using admin codes\"\n",
    "    )\n",
    "\n",
    "\n",
    "def create_admin_search_view(con):\n",
    "    \"\"\"Create a unified view for searching across all admin levels.\"\"\"\n",
    "\n",
    "    logger.debug(\"Creating unified admin search view...\")\n",
    "    con.execute(\"\"\"\n",
    "    CREATE OR REPLACE VIEW admin_search AS\n",
    "\n",
    "    -- Countries (admin0)\n",
    "    SELECT\n",
    "        geonameId,\n",
    "        name,\n",
    "        asciiname,\n",
    "        0 AS admin_level,\n",
    "        admin0_code,\n",
    "        NULL AS admin1_code,\n",
    "        NULL AS admin2_code,\n",
    "        NULL AS admin3_code,\n",
    "        NULL AS admin4_code,\n",
    "        NULL AS parent_name,\n",
    "        NULL AS parent_id,\n",
    "        feature_class,\n",
    "        feature_code,\n",
    "        Population AS population,\n",
    "        alternatenames,\n",
    "        'admin0' AS source_table\n",
    "    FROM\n",
    "        admin0\n",
    "\n",
    "    UNION ALL\n",
    "\n",
    "    -- Admin1 entities\n",
    "    SELECT\n",
    "        geonameId,\n",
    "        name,\n",
    "        asciiname,\n",
    "        1 AS admin_level,\n",
    "        admin0_code,\n",
    "        admin1_code,\n",
    "        NULL AS admin2_code,\n",
    "        NULL AS admin3_code,\n",
    "        NULL AS admin4_code,\n",
    "        parent_name,\n",
    "        parent_id,\n",
    "        feature_class,\n",
    "        feature_code,\n",
    "        population,\n",
    "        alternatenames,\n",
    "        'admin1' AS source_table\n",
    "    FROM\n",
    "        admin1\n",
    "\n",
    "    UNION ALL\n",
    "\n",
    "    -- Admin2 entities\n",
    "    SELECT\n",
    "        geonameId,\n",
    "        name,\n",
    "        asciiname,\n",
    "        2 AS admin_level,\n",
    "        admin0_code,\n",
    "        admin1_code,\n",
    "        admin2_code,\n",
    "        NULL AS admin3_code,\n",
    "        NULL AS admin4_code,\n",
    "        parent_name,\n",
    "        parent_id,\n",
    "        feature_class,\n",
    "        feature_code,\n",
    "        population,\n",
    "        alternatenames,\n",
    "        'admin2' AS source_table\n",
    "    FROM\n",
    "        admin2\n",
    "\n",
    "    UNION ALL\n",
    "\n",
    "    -- Admin3 entities\n",
    "    SELECT\n",
    "        geonameId,\n",
    "        name,\n",
    "        asciiname,\n",
    "        3 AS admin_level,\n",
    "        admin0_code,\n",
    "        admin1_code,\n",
    "        admin2_code,\n",
    "        admin3_code,\n",
    "        NULL AS admin4_code,\n",
    "        parent_name,\n",
    "        parent_id,\n",
    "        feature_class,\n",
    "        feature_code,\n",
    "        population,\n",
    "        alternatenames,\n",
    "        'admin3' AS source_table\n",
    "    FROM\n",
    "        admin3\n",
    "\n",
    "    UNION ALL\n",
    "\n",
    "    -- Admin4 entities\n",
    "    SELECT\n",
    "        geonameId,\n",
    "        name,\n",
    "        asciiname,\n",
    "        4 AS admin_level,\n",
    "        admin0_code,\n",
    "        admin1_code,\n",
    "        admin2_code,\n",
    "        admin3_code,\n",
    "        admin4_code,\n",
    "        parent_name,\n",
    "        parent_id,\n",
    "        feature_class,\n",
    "        feature_code,\n",
    "        population,\n",
    "        alternatenames,\n",
    "        'admin4' AS source_table\n",
    "    FROM\n",
    "        admin4\n",
    "    \"\"\")\n",
    "\n",
    "    logger.debug(\"Admin search view created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_admin_tables_hybrid(con, conn, overwrite=True):\n",
    "    \"\"\"Build admin tables using graph database for relationships and DuckDB for storage.\"\"\"\n",
    "\n",
    "    logger.debug(\"Starting hybrid admin table construction...\")\n",
    "\n",
    "    # Define admin level feature code patterns\n",
    "    level_codes = {\n",
    "        0: [\"PCL\", \"PCLI\", \"PCLD\", \"PCLF\", \"PCLS\", \"TERR\"],\n",
    "        1: [\"ADM1\", \"ADM1H\"],\n",
    "        2: [\"ADM2\", \"ADM2H\"],\n",
    "        3: [\"ADM3\", \"ADM3H\"],\n",
    "        4: [\"ADM4\", \"ADM4H\"],\n",
    "    }\n",
    "\n",
    "    # Track entities at each admin level\n",
    "    admin_entities = {}\n",
    "\n",
    "    # Build admin tables one by one\n",
    "    for level in range(0, 5):\n",
    "        table_name = f\"admin{level}\"\n",
    "\n",
    "        if table_exists(con, table_name) and not overwrite:\n",
    "            logger.debug(f\"Table {table_name} already exists. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        logger.info(f\"Building {table_name} table...\")\n",
    "\n",
    "        # Step 1: Identify entities of this admin level by feature code\n",
    "        feature_patterns = \"', '\".join([code for code in level_codes[level]])\n",
    "        base_entities_query = f\"\"\"\n",
    "            SELECT geonameId\n",
    "            FROM allCountries\n",
    "            WHERE feature_code IN ('{feature_patterns}')\n",
    "            OR feature_code LIKE '{level_codes[level][0]}%'\n",
    "        \"\"\"\n",
    "\n",
    "        level_entities = set(con.execute(base_entities_query).pl()[\"geonameId\"])\n",
    "        logger.debug(f\"Found {len(level_entities)} initial entities for level {level}\")\n",
    "\n",
    "        # Step 2: For levels 1-4, use graph DB to validate hierarchical placement\n",
    "        # This ensures entities are correctly placed in the admin hierarchy\n",
    "        if level > 0 and level - 1 in admin_entities:\n",
    "            # Find all direct children of the previous level's entities\n",
    "            previous_level_ids = list(admin_entities[level - 1])\n",
    "\n",
    "            # Use graph DB to get valid children\n",
    "            children_query = f\"\"\"\n",
    "            MATCH (p:Entity)-[:IsIn]->(c:Entity)\n",
    "            WHERE p.geonameId IN CAST({previous_level_ids}, \"UINT32[]\")\n",
    "            RETURN DISTINCT c.geonameId AS geonameId\n",
    "            \"\"\"\n",
    "\n",
    "            valid_children = set(conn.execute(children_query).get_as_pl()[\"geonameId\"])\n",
    "            logger.debug(\n",
    "                f\"Found {len(valid_children)} valid children from graph relationships\"\n",
    "            )\n",
    "\n",
    "            # Combine feature-based and relationship-based entities\n",
    "            level_entities = level_entities.union(valid_children)\n",
    "\n",
    "        admin_entities[level] = level_entities\n",
    "\n",
    "        # Step 3: Create the admin table with appropriate fields\n",
    "        if level == 0:  # Special case for countries\n",
    "            create_query = f\"\"\"\n",
    "            CREATE OR REPLACE TABLE {table_name} AS\n",
    "            SELECT\n",
    "                a.geonameId,\n",
    "                a.name,\n",
    "                a.asciiname,\n",
    "                a.admin0_code,\n",
    "                a.feature_class,\n",
    "                a.feature_code,\n",
    "                c.ISO,\n",
    "                c.ISO3,\n",
    "                c.ISO_Numeric,\n",
    "                c.Country AS official_name,\n",
    "                c.fips,\n",
    "                c.Population,\n",
    "                c.Area,\n",
    "                a.alternatenames,\n",
    "                NULL AS parent_id  -- No parent for countries\n",
    "            FROM\n",
    "                allCountries a\n",
    "            LEFT JOIN\n",
    "                countryInfo c ON a.geonameId = c.geonameId\n",
    "            WHERE\n",
    "                a.geonameId IN ({\",\".join(map(str, level_entities))})\n",
    "            ORDER BY\n",
    "                a.geonameId\n",
    "            \"\"\"\n",
    "        else:\n",
    "            # For admin levels 1-4, include parent relationship\n",
    "            create_query = f\"\"\"\n",
    "            CREATE OR REPLACE TABLE {table_name} AS\n",
    "            WITH parent_links AS (\n",
    "                SELECT\n",
    "                    childId AS geonameId,\n",
    "                    parentId AS parent_id\n",
    "                FROM\n",
    "                    hierarchy\n",
    "                WHERE\n",
    "                    type = 'ADM' AND\n",
    "                    childId IN ({\",\".join(map(str, level_entities))})\n",
    "            )\n",
    "            SELECT\n",
    "                a.geonameId,\n",
    "                a.name,\n",
    "                a.asciiname,\n",
    "                a.admin0_code\n",
    "                {\", a.admin1_code\" if level >= 1 else \", NULL AS admin1_code\"}\n",
    "                {\", a.admin2_code\" if level >= 2 else \", NULL AS admin2_code\"}\n",
    "                {\", a.admin3_code\" if level >= 3 else \", NULL AS admin3_code\"}\n",
    "                {\", a.admin4_code\" if level >= 4 else \", NULL AS admin4_code\"},\n",
    "                a.feature_class,\n",
    "                a.feature_code,\n",
    "                a.population,\n",
    "                p.parent_id,\n",
    "                c.name AS country_name,\n",
    "                CASE\n",
    "                    WHEN p.parent_id IS NOT NULL THEN parent.name\n",
    "                    ELSE NULL\n",
    "                END AS parent_name,\n",
    "                a.alternatenames\n",
    "            FROM\n",
    "                allCountries a\n",
    "            LEFT JOIN\n",
    "                parent_links p ON a.geonameId = p.geonameId\n",
    "            LEFT JOIN\n",
    "                allCountries parent ON p.parent_id = parent.geonameId\n",
    "            LEFT JOIN\n",
    "                admin0 c ON a.admin0_code = c.admin0_code\n",
    "            WHERE\n",
    "                a.geonameId IN ({\",\".join(map(str, level_entities))})\n",
    "            ORDER BY\n",
    "                a.geonameId\n",
    "            \"\"\"\n",
    "\n",
    "        # Execute the query to create the table\n",
    "        con.execute(create_query)\n",
    "\n",
    "        # Step 4: Add indexes and FTS\n",
    "        con.execute(f\"CREATE INDEX idx_{table_name}_gid ON {table_name} (geonameId)\")\n",
    "\n",
    "        if level > 0:\n",
    "            # Create indexes for parent relationships\n",
    "            con.execute(\n",
    "                f\"CREATE INDEX idx_{table_name}_parent ON {table_name} (parent_id)\"\n",
    "            )\n",
    "            con.execute(\n",
    "                f\"CREATE INDEX idx_{table_name}_admin0 ON {table_name} (admin0_code)\"\n",
    "            )\n",
    "\n",
    "            # Create admin code index specific to this level\n",
    "            if level <= 4:\n",
    "                con.execute(\n",
    "                    f\"CREATE INDEX idx_{table_name}_code ON {table_name} (admin{level}_code)\"\n",
    "                )\n",
    "\n",
    "        # Create FTS index for searching\n",
    "        fts_fields = \"geonameId, name, asciiname, alternatenames\"\n",
    "        if level == 0:\n",
    "            fts_fields += \", official_name, ISO, ISO3\"\n",
    "\n",
    "        con.execute(f\"\"\"\n",
    "        PRAGMA create_fts_index(\n",
    "            {table_name},\n",
    "            {fts_fields},\n",
    "            stemmer = 'none',\n",
    "            stopwords = 'none',\n",
    "            ignore = '(\\\\.|[^a-z0-9])+',\n",
    "            overwrite = 1\n",
    "        )\n",
    "        \"\"\")\n",
    "\n",
    "        # Report count\n",
    "        count = con.execute(f\"SELECT COUNT(*) FROM {table_name}\").fetchone()[0]\n",
    "        logger.debug(f\"Created {table_name} with {count} entities\")\n",
    "\n",
    "    # Step 5: Handle orphaned entities\n",
    "    logger.debug(\"\\nChecking for orphaned administrative entities...\")\n",
    "    handle_orphaned_admin_entities(con, conn)\n",
    "\n",
    "    # Step 6: Create a unified admin search view\n",
    "    create_admin_search_view(con)\n",
    "    # Admin1 composite index\n",
    "    con.execute(\n",
    "        \"CREATE INDEX IF NOT EXISTS admin1_codes ON admin1 (admin0_code, admin1_code)\"\n",
    "    )\n",
    "\n",
    "    # Admin2 composite index\n",
    "    con.execute(\n",
    "        \"CREATE INDEX IF NOT EXISTS admin2_codes ON admin2 (admin0_code, admin1_code, admin2_code)\"\n",
    "    )\n",
    "\n",
    "    # Admin3 composite index\n",
    "    con.execute(\n",
    "        \"CREATE INDEX IF NOT EXISTS admin3_codes ON admin3 (admin0_code, admin1_code, admin2_code, admin3_code)\"\n",
    "    )\n",
    "\n",
    "    # Admin4 composite index\n",
    "    con.execute(\n",
    "        \"CREATE INDEX IF NOT EXISTS admin4_codes ON admin4 (admin0_code, admin1_code, admin2_code, admin3_code, admin4_code)\"\n",
    "    )\n",
    "\n",
    "    logger.debug(\"Created composite indexes on admin code columns\")\n",
    "\n",
    "    logger.debug(\"Hybrid admin table construction complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-04-29 14:07:40.727\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mbuild_admin_tables_hybrid\u001b[0m:\u001b[36m4\u001b[0m - \u001b[34m\u001b[1mStarting hybrid admin table construction...\u001b[0m\n",
      "\u001b[32m2025-04-29 14:07:40.732\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mbuild_admin_tables_hybrid\u001b[0m:\u001b[36m26\u001b[0m - \u001b[1mBuilding admin0 table...\u001b[0m\n",
      "\u001b[32m2025-04-29 14:07:40.812\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mbuild_admin_tables_hybrid\u001b[0m:\u001b[36m38\u001b[0m - \u001b[34m\u001b[1mFound 271 initial entities for level 0\u001b[0m\n",
      "\u001b[32m2025-04-29 14:07:40.935\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mbuild_admin_tables_hybrid\u001b[0m:\u001b[36m178\u001b[0m - \u001b[34m\u001b[1mCreated admin0 with 271 entities\u001b[0m\n",
      "\u001b[32m2025-04-29 14:07:40.938\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mbuild_admin_tables_hybrid\u001b[0m:\u001b[36m26\u001b[0m - \u001b[1mBuilding admin1 table...\u001b[0m\n",
      "\u001b[32m2025-04-29 14:07:40.961\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mbuild_admin_tables_hybrid\u001b[0m:\u001b[36m38\u001b[0m - \u001b[34m\u001b[1mFound 4500 initial entities for level 1\u001b[0m\n",
      "\u001b[32m2025-04-29 14:07:40.990\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mbuild_admin_tables_hybrid\u001b[0m:\u001b[36m54\u001b[0m - \u001b[34m\u001b[1mFound 4014 valid children from graph relationships\u001b[0m\n",
      "\u001b[32m2025-04-29 14:07:41.728\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mbuild_admin_tables_hybrid\u001b[0m:\u001b[36m178\u001b[0m - \u001b[34m\u001b[1mCreated admin1 with 4924 entities\u001b[0m\n",
      "\u001b[32m2025-04-29 14:07:41.731\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mbuild_admin_tables_hybrid\u001b[0m:\u001b[36m26\u001b[0m - \u001b[1mBuilding admin2 table...\u001b[0m\n",
      "\u001b[32m2025-04-29 14:07:41.754\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mbuild_admin_tables_hybrid\u001b[0m:\u001b[36m38\u001b[0m - \u001b[34m\u001b[1mFound 48830 initial entities for level 2\u001b[0m\n",
      "\u001b[32m2025-04-29 14:07:42.072\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mbuild_admin_tables_hybrid\u001b[0m:\u001b[36m54\u001b[0m - \u001b[34m\u001b[1mFound 48045 valid children from graph relationships\u001b[0m\n",
      "\u001b[32m2025-04-29 14:07:43.875\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mbuild_admin_tables_hybrid\u001b[0m:\u001b[36m178\u001b[0m - \u001b[34m\u001b[1mCreated admin2 with 54970 entities\u001b[0m\n",
      "\u001b[32m2025-04-29 14:07:43.877\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mbuild_admin_tables_hybrid\u001b[0m:\u001b[36m26\u001b[0m - \u001b[1mBuilding admin3 table...\u001b[0m\n",
      "\u001b[32m2025-04-29 14:07:43.908\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mbuild_admin_tables_hybrid\u001b[0m:\u001b[36m38\u001b[0m - \u001b[34m\u001b[1mFound 169901 initial entities for level 3\u001b[0m\n",
      "\u001b[32m2025-04-29 14:07:47.103\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mbuild_admin_tables_hybrid\u001b[0m:\u001b[36m54\u001b[0m - \u001b[34m\u001b[1mFound 158416 valid children from graph relationships\u001b[0m\n",
      "\u001b[32m2025-04-29 14:07:50.944\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mbuild_admin_tables_hybrid\u001b[0m:\u001b[36m178\u001b[0m - \u001b[34m\u001b[1mCreated admin3 with 238152 entities\u001b[0m\n",
      "\u001b[32m2025-04-29 14:07:50.947\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mbuild_admin_tables_hybrid\u001b[0m:\u001b[36m26\u001b[0m - \u001b[1mBuilding admin4 table...\u001b[0m\n",
      "\u001b[32m2025-04-29 14:07:50.978\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mbuild_admin_tables_hybrid\u001b[0m:\u001b[36m38\u001b[0m - \u001b[34m\u001b[1mFound 227091 initial entities for level 4\u001b[0m\n",
      "\u001b[32m2025-04-29 14:08:01.997\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mbuild_admin_tables_hybrid\u001b[0m:\u001b[36m54\u001b[0m - \u001b[34m\u001b[1mFound 112452 valid children from graph relationships\u001b[0m\n",
      "\u001b[32m2025-04-29 14:08:06.571\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mbuild_admin_tables_hybrid\u001b[0m:\u001b[36m178\u001b[0m - \u001b[34m\u001b[1mCreated admin4 with 270859 entities\u001b[0m\n",
      "\u001b[32m2025-04-29 14:08:06.572\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mbuild_admin_tables_hybrid\u001b[0m:\u001b[36m181\u001b[0m - \u001b[34m\u001b[1m\n",
      "Checking for orphaned administrative entities...\u001b[0m\n",
      "\u001b[32m2025-04-29 14:08:06.677\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mhandle_orphaned_admin_entities\u001b[0m:\u001b[36m23\u001b[0m - \u001b[34m\u001b[1mFound 90155 orphaned admin entities.\u001b[0m\n",
      "\u001b[32m2025-04-29 14:08:12.523\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mhandle_orphaned_admin_entities\u001b[0m:\u001b[36m94\u001b[0m - \u001b[34m\u001b[1mFixed 14952 orphans using graph relationships and 0 using admin codes\u001b[0m\n",
      "\u001b[32m2025-04-29 14:08:12.524\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mcreate_admin_search_view\u001b[0m:\u001b[36m102\u001b[0m - \u001b[34m\u001b[1mCreating unified admin search view...\u001b[0m\n",
      "\u001b[32m2025-04-29 14:08:12.526\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mcreate_admin_search_view\u001b[0m:\u001b[36m220\u001b[0m - \u001b[34m\u001b[1mAdmin search view created!\u001b[0m\n",
      "\u001b[32m2025-04-29 14:08:12.838\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mbuild_admin_tables_hybrid\u001b[0m:\u001b[36m206\u001b[0m - \u001b[34m\u001b[1mCreated composite indexes on admin code columns\u001b[0m\n",
      "\u001b[32m2025-04-29 14:08:12.838\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mbuild_admin_tables_hybrid\u001b[0m:\u001b[36m208\u001b[0m - \u001b[34m\u001b[1mHybrid admin table construction complete!\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "build_admin_tables_hybrid(con, conn, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (4_924, 15)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>geonameId</th><th>name</th><th>asciiname</th><th>admin0_code</th><th>admin1_code</th><th>admin2_code</th><th>admin3_code</th><th>admin4_code</th><th>feature_class</th><th>feature_code</th><th>population</th><th>parent_id</th><th>country_name</th><th>parent_name</th><th>alternatenames</th></tr><tr><td>i32</td><td>str</td><td>str</td><td>str</td><td>str</td><td>i32</td><td>i32</td><td>i32</td><td>str</td><td>str</td><td>i64</td><td>i32</td><td>str</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>432</td><td>&quot;Kūh-e Qal‘eh Toshak&quot;</td><td>&quot;Kuh-e Qal`eh Toshak&quot;</td><td>&quot;IR&quot;</td><td>&quot;35&quot;</td><td>null</td><td>null</td><td>null</td><td>&quot;T&quot;</td><td>&quot;MT&quot;</td><td>0</td><td>null</td><td>&quot;Islamic Republic of Iran&quot;</td><td>null</td><td>&quot;Kuh-e Ab-e Sefid,Kuh-e Qal`eh …</td></tr><tr><td>50360</td><td>&quot;Gobolka Woqooyi Galbeed&quot;</td><td>&quot;Gobolka Woqooyi Galbeed&quot;</td><td>&quot;SO&quot;</td><td>&quot;20&quot;</td><td>null</td><td>null</td><td>null</td><td>&quot;A&quot;</td><td>&quot;ADM1&quot;</td><td>1240000</td><td>51537</td><td>&quot;Somalia&quot;</td><td>&quot;Somalia&quot;</td><td>&quot;Gobolka Woqooyi Galbeed,Hargei…</td></tr><tr><td>51230</td><td>&quot;Gobolka Togdheer&quot;</td><td>&quot;Gobolka Togdheer&quot;</td><td>&quot;SO&quot;</td><td>&quot;19&quot;</td><td>null</td><td>null</td><td>null</td><td>&quot;A&quot;</td><td>&quot;ADM1&quot;</td><td>350000</td><td>51537</td><td>&quot;Somalia&quot;</td><td>&quot;Somalia&quot;</td><td>&quot;Burao,Dohdayr,Gobolka Togdheer…</td></tr><tr><td>51966</td><td>&quot;Gobolka Shabeellaha Hoose&quot;</td><td>&quot;Gobolka Shabeellaha Hoose&quot;</td><td>&quot;SO&quot;</td><td>&quot;14&quot;</td><td>null</td><td>null</td><td>null</td><td>&quot;A&quot;</td><td>&quot;ADM1&quot;</td><td>1200000</td><td>51537</td><td>&quot;Somalia&quot;</td><td>&quot;Somalia&quot;</td><td>&quot;Gobolka Shabeellaha Hoose,Lowe…</td></tr><tr><td>51967</td><td>&quot;Gobolka Shabeellaha Dhexe&quot;</td><td>&quot;Gobolka Shabeellaha Dhexe&quot;</td><td>&quot;SO&quot;</td><td>&quot;13&quot;</td><td>null</td><td>null</td><td>null</td><td>&quot;A&quot;</td><td>&quot;ADM1&quot;</td><td>516000</td><td>51537</td><td>&quot;Somalia&quot;</td><td>&quot;Somalia&quot;</td><td>&quot;Central Shabele,Gobolka Shabee…</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>12902747</td><td>&quot;Lim-Pendé&quot;</td><td>&quot;Lim-Pende&quot;</td><td>&quot;CF&quot;</td><td>&quot;21&quot;</td><td>null</td><td>null</td><td>null</td><td>&quot;A&quot;</td><td>&quot;ADM1&quot;</td><td>442151</td><td>239880</td><td>&quot;Central African Republic&quot;</td><td>&quot;Central African Republic&quot;</td><td>null</td></tr><tr><td>12902749</td><td>&quot;Central Ethiopia Regional Stat…</td><td>&quot;Central Ethiopia Regional Stat…</td><td>&quot;ET&quot;</td><td>&quot;55&quot;</td><td>null</td><td>null</td><td>null</td><td>&quot;A&quot;</td><td>&quot;ADM1&quot;</td><td>9382700</td><td>337996</td><td>&quot;Federal Democratic Republic of…</td><td>&quot;Federal Democratic Republic of…</td><td>null</td></tr><tr><td>12902766</td><td>&quot;South Ethiopia Regional State&quot;</td><td>&quot;South Ethiopia Regional State&quot;</td><td>&quot;ET&quot;</td><td>&quot;56&quot;</td><td>null</td><td>null</td><td>null</td><td>&quot;A&quot;</td><td>&quot;ADM1&quot;</td><td>7728100</td><td>337996</td><td>&quot;Federal Democratic Republic of…</td><td>&quot;Federal Democratic Republic of…</td><td>null</td></tr><tr><td>12902784</td><td>&quot;South West Ethiopia Peoples&#x27; R…</td><td>&quot;South West Ethiopia Peoples&#x27; R…</td><td>&quot;ET&quot;</td><td>&quot;SW&quot;</td><td>null</td><td>null</td><td>null</td><td>&quot;A&quot;</td><td>&quot;ADM1&quot;</td><td>2437200</td><td>337996</td><td>&quot;Federal Democratic Republic of…</td><td>&quot;Federal Democratic Republic of…</td><td>null</td></tr><tr><td>13118168</td><td>&quot;Vallée de la Garonne&quot;</td><td>&quot;Vallee de la Garonne&quot;</td><td>&quot;FR&quot;</td><td>&quot;00&quot;</td><td>null</td><td>null</td><td>null</td><td>&quot;T&quot;</td><td>&quot;VAL&quot;</td><td>0</td><td>null</td><td>&quot;Republic of France&quot;</td><td>null</td><td>null</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (4_924, 15)\n",
       "┌───────────┬───────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬──────────┐\n",
       "│ geonameId ┆ name      ┆ asciiname ┆ admin0_co ┆ … ┆ parent_id ┆ country_n ┆ parent_na ┆ alternat │\n",
       "│ ---       ┆ ---       ┆ ---       ┆ de        ┆   ┆ ---       ┆ ame       ┆ me        ┆ enames   │\n",
       "│ i32       ┆ str       ┆ str       ┆ ---       ┆   ┆ i32       ┆ ---       ┆ ---       ┆ ---      │\n",
       "│           ┆           ┆           ┆ str       ┆   ┆           ┆ str       ┆ str       ┆ str      │\n",
       "╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪══════════╡\n",
       "│ 432       ┆ Kūh-e     ┆ Kuh-e     ┆ IR        ┆ … ┆ null      ┆ Islamic   ┆ null      ┆ Kuh-e    │\n",
       "│           ┆ Qal‘eh    ┆ Qal`eh    ┆           ┆   ┆           ┆ Republic  ┆           ┆ Ab-e Sef │\n",
       "│           ┆ Toshak    ┆ Toshak    ┆           ┆   ┆           ┆ of Iran   ┆           ┆ id,Kuh-e │\n",
       "│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ Qal`eh … │\n",
       "│ 50360     ┆ Gobolka   ┆ Gobolka   ┆ SO        ┆ … ┆ 51537     ┆ Somalia   ┆ Somalia   ┆ Gobolka  │\n",
       "│           ┆ Woqooyi   ┆ Woqooyi   ┆           ┆   ┆           ┆           ┆           ┆ Woqooyi  │\n",
       "│           ┆ Galbeed   ┆ Galbeed   ┆           ┆   ┆           ┆           ┆           ┆ Galbeed, │\n",
       "│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ Hargei…  │\n",
       "│ 51230     ┆ Gobolka   ┆ Gobolka   ┆ SO        ┆ … ┆ 51537     ┆ Somalia   ┆ Somalia   ┆ Burao,Do │\n",
       "│           ┆ Togdheer  ┆ Togdheer  ┆           ┆   ┆           ┆           ┆           ┆ hdayr,Go │\n",
       "│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ bolka    │\n",
       "│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ Togdheer │\n",
       "│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ …        │\n",
       "│ 51966     ┆ Gobolka   ┆ Gobolka   ┆ SO        ┆ … ┆ 51537     ┆ Somalia   ┆ Somalia   ┆ Gobolka  │\n",
       "│           ┆ Shabeella ┆ Shabeella ┆           ┆   ┆           ┆           ┆           ┆ Shabeell │\n",
       "│           ┆ ha Hoose  ┆ ha Hoose  ┆           ┆   ┆           ┆           ┆           ┆ aha Hoos │\n",
       "│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ e,Lowe…  │\n",
       "│ 51967     ┆ Gobolka   ┆ Gobolka   ┆ SO        ┆ … ┆ 51537     ┆ Somalia   ┆ Somalia   ┆ Central  │\n",
       "│           ┆ Shabeella ┆ Shabeella ┆           ┆   ┆           ┆           ┆           ┆ Shabele, │\n",
       "│           ┆ ha Dhexe  ┆ ha Dhexe  ┆           ┆   ┆           ┆           ┆           ┆ Gobolka  │\n",
       "│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ Shabee…  │\n",
       "│ …         ┆ …         ┆ …         ┆ …         ┆ … ┆ …         ┆ …         ┆ …         ┆ …        │\n",
       "│ 12902747  ┆ Lim-Pendé ┆ Lim-Pende ┆ CF        ┆ … ┆ 239880    ┆ Central   ┆ Central   ┆ null     │\n",
       "│           ┆           ┆           ┆           ┆   ┆           ┆ African   ┆ African   ┆          │\n",
       "│           ┆           ┆           ┆           ┆   ┆           ┆ Republic  ┆ Republic  ┆          │\n",
       "│ 12902749  ┆ Central   ┆ Central   ┆ ET        ┆ … ┆ 337996    ┆ Federal   ┆ Federal   ┆ null     │\n",
       "│           ┆ Ethiopia  ┆ Ethiopia  ┆           ┆   ┆           ┆ Democrati ┆ Democrati ┆          │\n",
       "│           ┆ Regional  ┆ Regional  ┆           ┆   ┆           ┆ c         ┆ c         ┆          │\n",
       "│           ┆ Stat…     ┆ Stat…     ┆           ┆   ┆           ┆ Republic  ┆ Republic  ┆          │\n",
       "│           ┆           ┆           ┆           ┆   ┆           ┆ of…       ┆ of…       ┆          │\n",
       "│ 12902766  ┆ South     ┆ South     ┆ ET        ┆ … ┆ 337996    ┆ Federal   ┆ Federal   ┆ null     │\n",
       "│           ┆ Ethiopia  ┆ Ethiopia  ┆           ┆   ┆           ┆ Democrati ┆ Democrati ┆          │\n",
       "│           ┆ Regional  ┆ Regional  ┆           ┆   ┆           ┆ c         ┆ c         ┆          │\n",
       "│           ┆ State     ┆ State     ┆           ┆   ┆           ┆ Republic  ┆ Republic  ┆          │\n",
       "│           ┆           ┆           ┆           ┆   ┆           ┆ of…       ┆ of…       ┆          │\n",
       "│ 12902784  ┆ South     ┆ South     ┆ ET        ┆ … ┆ 337996    ┆ Federal   ┆ Federal   ┆ null     │\n",
       "│           ┆ West      ┆ West      ┆           ┆   ┆           ┆ Democrati ┆ Democrati ┆          │\n",
       "│           ┆ Ethiopia  ┆ Ethiopia  ┆           ┆   ┆           ┆ c         ┆ c         ┆          │\n",
       "│           ┆ Peoples'  ┆ Peoples'  ┆           ┆   ┆           ┆ Republic  ┆ Republic  ┆          │\n",
       "│           ┆ R…        ┆ R…        ┆           ┆   ┆           ┆ of…       ┆ of…       ┆          │\n",
       "│ 13118168  ┆ Vallée de ┆ Vallee de ┆ FR        ┆ … ┆ null      ┆ Republic  ┆ null      ┆ null     │\n",
       "│           ┆ la        ┆ la        ┆           ┆   ┆           ┆ of France ┆           ┆          │\n",
       "│           ┆ Garonne   ┆ Garonne   ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "└───────────┴───────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴──────────┘"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con.table(\"admin1\").pl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-04-29 15:35:51.712\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mhierarchical_search\u001b[0m:\u001b[36m333\u001b[0m - \u001b[34m\u001b[1mSearching for country: 'FR'\u001b[0m\n",
      "\u001b[32m2025-04-29 15:35:51.736\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mhierarchical_search\u001b[0m:\u001b[36m341\u001b[0m - \u001b[34m\u001b[1mSearching for admin1: 'Provence-Alpes-Côte d'Azur'\u001b[0m\n",
      "\u001b[32m2025-04-29 15:35:51.758\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36madmin_score\u001b[0m:\u001b[36m48\u001b[0m - \u001b[34m\u001b[1mFound parent score column: adjusted_score_0\u001b[0m\n",
      "\u001b[32m2025-04-29 15:35:51.758\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36madmin_score\u001b[0m:\u001b[36m54\u001b[0m - \u001b[34m\u001b[1mUsing parent score from level 0: adjusted_score_0\u001b[0m\n",
      "\u001b[32m2025-04-29 15:35:51.759\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mhierarchical_search\u001b[0m:\u001b[36m371\u001b[0m - \u001b[34m\u001b[1mSearching for admin4: 'Le Lavandou'\u001b[0m\n",
      "\u001b[32m2025-04-29 15:35:51.790\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36madmin_score\u001b[0m:\u001b[36m48\u001b[0m - \u001b[34m\u001b[1mFound parent score column: adjusted_score_1\u001b[0m\n",
      "\u001b[32m2025-04-29 15:35:51.791\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36madmin_score\u001b[0m:\u001b[36m54\u001b[0m - \u001b[34m\u001b[1mUsing parent score from level 1: adjusted_score_1\u001b[0m\n",
      "\u001b[32m2025-04-29 15:35:51.792\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m390\u001b[0m - \u001b[34m\u001b[1mCountry results:\u001b[0m\n",
      "\u001b[32m2025-04-29 15:35:51.792\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m393\u001b[0m - \u001b[34m\u001b[1mAdmin1 results:\u001b[0m\n",
      "\u001b[32m2025-04-29 15:35:51.793\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m406\u001b[0m - \u001b[34m\u001b[1mAdmin4 results:\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Highest searched level: 0 (with 'adjusted_score_0')\n",
      "Filtering with path conditions: (admin0_code = 'GF') OR (admin0_code = 'MF') OR (admin0_code = 'PF') OR (admin0_code = 'FR')\n",
      "Added parent score from adjusted_score_0\n",
      "Highest searched level: 1 (with 'adjusted_score_1')\n",
      "Filtering with path conditions: (admin0_code = 'FR' AND admin1_code = '52') OR (admin0_code = 'FR' AND admin1_code = '93') OR (admin0_code = 'FR' AND admin1_code = 'B9') OR (admin0_code = 'FR' AND admin1_code = '84') OR (admin0_code = 'FR' AND admin1_code = 'B4') OR (admin0_code = 'FR' AND admin1_code = '32') OR (admin0_code = 'FR' AND admin1_code = '24') OR (admin0_code = 'FR' AND admin1_code = '11')\n",
      "Added parent score from adjusted_score_1\n",
      "shape: (4, 6)\n",
      "┌───────────┬────────────────────┬───────────────┬──────────────┬─────────────┬──────────────────┐\n",
      "│ geonameId ┆ name               ┆ feature_class ┆ feature_code ┆ admin0_code ┆ adjusted_score_0 │\n",
      "│ ---       ┆ ---                ┆ ---           ┆ ---          ┆ ---         ┆ ---              │\n",
      "│ i32       ┆ str                ┆ str           ┆ str          ┆ str         ┆ f64              │\n",
      "╞═══════════╪════════════════════╪═══════════════╪══════════════╪═════════════╪══════════════════╡\n",
      "│ 3017382   ┆ Republic of France ┆ A             ┆ PCLI         ┆ FR          ┆ 35.694998        │\n",
      "│ 3578421   ┆ Saint-Martin       ┆ A             ┆ PCLIX        ┆ MF          ┆ 6.800223         │\n",
      "│ 3381670   ┆ Guyane             ┆ A             ┆ PCLD         ┆ GF          ┆ 4.640878         │\n",
      "│ 4030656   ┆ French Polynesia   ┆ A             ┆ PCLD         ┆ PF          ┆ 4.148781         │\n",
      "└───────────┴────────────────────┴───────────────┴──────────────┴─────────────┴──────────────────┘\n",
      "shape: (10, 11)\n",
      "┌───────────┬───────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬──────────┐\n",
      "│ geonameId ┆ name      ┆ feature_c ┆ feature_c ┆ … ┆ admin3_co ┆ admin4_co ┆ adjusted_ ┆ adjusted │\n",
      "│ ---       ┆ ---       ┆ lass      ┆ ode       ┆   ┆ de        ┆ de        ┆ score_0   ┆ _score_1 │\n",
      "│ i32       ┆ str       ┆ ---       ┆ ---       ┆   ┆ ---       ┆ ---       ┆ ---       ┆ ---      │\n",
      "│           ┆           ┆ str       ┆ str       ┆   ┆ i32       ┆ i32       ┆ f64       ┆ f64      │\n",
      "╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪══════════╡\n",
      "│ 2985244   ┆ Provence- ┆ A         ┆ ADM1      ┆ … ┆ null      ┆ null      ┆ 35.694998 ┆ 39.59096 │\n",
      "│           ┆ Alpes-Côt ┆           ┆           ┆   ┆           ┆           ┆           ┆ 3        │\n",
      "│           ┆ e d'Azur  ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
      "│ 2983751   ┆ Rhône-Alp ┆ A         ┆ ADM1H     ┆ … ┆ null      ┆ null      ┆ 35.694998 ┆ 13.57186 │\n",
      "│           ┆ es        ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
      "│ 11071625  ┆ Auvergne- ┆ A         ┆ ADM1      ┆ … ┆ null      ┆ null      ┆ 35.694998 ┆ 13.23396 │\n",
      "│           ┆ Rhône-Alp ┆           ┆           ┆   ┆           ┆           ┆           ┆ 7        │\n",
      "│           ┆ es        ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
      "│ 8378478   ┆ Alpes     ┆ A         ┆ ADM1H     ┆ … ┆ null      ┆ null      ┆ 35.694998 ┆ 9.547392 │\n",
      "│           ┆ Maritimae ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
      "│ 8378477   ┆ Alpes     ┆ A         ┆ ADM1H     ┆ … ┆ null      ┆ null      ┆ 35.694998 ┆ 9.481978 │\n",
      "│           ┆ Graiae    ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
      "│ 3027939   ┆ Centre-Va ┆ A         ┆ ADM1      ┆ … ┆ null      ┆ null      ┆ 35.694998 ┆ 4.925064 │\n",
      "│           ┆ l de      ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
      "│           ┆ Loire     ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
      "│ 11071624  ┆ Hauts-de- ┆ A         ┆ ADM1      ┆ … ┆ null      ┆ null      ┆ 35.694998 ┆ 4.738544 │\n",
      "│           ┆ France    ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
      "│ 2988289   ┆ Pays de   ┆ A         ┆ ADM1      ┆ … ┆ null      ┆ null      ┆ 35.694998 ┆ 4.68193  │\n",
      "│           ┆ la Loire  ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
      "│ 3012874   ┆ Île-de-Fr ┆ A         ┆ ADM1      ┆ … ┆ null      ┆ null      ┆ 35.694998 ┆ 3.341229 │\n",
      "│           ┆ ance      ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
      "│ 2990119   ┆ Nord-Pas- ┆ A         ┆ ADM1H     ┆ … ┆ null      ┆ null      ┆ 35.694998 ┆ 3.127863 │\n",
      "│           ┆ de-Calais ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
      "└───────────┴───────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴──────────┘\n",
      "shape: (10, 11)\n",
      "┌───────────┬───────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬──────────┐\n",
      "│ geonameId ┆ name      ┆ feature_c ┆ feature_c ┆ … ┆ admin3_co ┆ admin4_co ┆ adjusted_ ┆ adjusted │\n",
      "│ ---       ┆ ---       ┆ lass      ┆ ode       ┆   ┆ de        ┆ de        ┆ score_1   ┆ _score_4 │\n",
      "│ i32       ┆ str       ┆ ---       ┆ ---       ┆   ┆ ---       ┆ ---       ┆ ---       ┆ ---      │\n",
      "│           ┆           ┆ str       ┆ str       ┆   ┆ str       ┆ str       ┆ f64       ┆ f64      │\n",
      "╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪══════════╡\n",
      "│ 6615009   ┆ Le        ┆ A         ┆ ADM4      ┆ … ┆ 832       ┆ 83070     ┆ 9.547392  ┆ 13.54434 │\n",
      "│           ┆ Lavandou  ┆           ┆           ┆   ┆           ┆           ┆           ┆ 5        │\n",
      "│ 6455417   ┆ Le        ┆ A         ┆ ADM4      ┆ … ┆ 783       ┆ 78650     ┆ 9.547392  ┆ 5.778189 │\n",
      "│           ┆ Vésinet   ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
      "│ 6456500   ┆ Le Mesnil ┆ A         ┆ ADM4      ┆ … ┆ 783       ┆ 78396     ┆ 9.547392  ┆ 5.767837 │\n",
      "│           ┆ -le-Roi   ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
      "│ 6455917   ┆ Le Poinço ┆ A         ┆ ADM4      ┆ … ┆ 362       ┆ 36159     ┆ 9.547392  ┆ 5.653263 │\n",
      "│           ┆ nnet      ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
      "│ 6456350   ┆ Le        ┆ A         ┆ ADM4      ┆ … ┆ 692       ┆ 69151     ┆ 9.547392  ┆ 5.548559 │\n",
      "│           ┆ Perréon   ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
      "│ 6455915   ┆ Le        ┆ A         ┆ ADM4      ┆ … ┆ 362       ┆ 36154     ┆ 9.547392  ┆ 5.508995 │\n",
      "│           ┆ Pêchereau ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
      "│ 6456917   ┆ Le Gâvre  ┆ A         ┆ ADM4      ┆ … ┆ 445       ┆ 44062     ┆ 9.547392  ┆ 5.503586 │\n",
      "│ 6456492   ┆ Le        ┆ A         ┆ ADM4      ┆ … ┆ 774       ┆ 77485     ┆ 9.547392  ┆ 5.393844 │\n",
      "│           ┆ Vaudoué   ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
      "│ 6455921   ┆ Le Liège  ┆ A         ┆ ADM4      ┆ … ┆ 373       ┆ 37127     ┆ 9.547392  ┆ 5.300726 │\n",
      "│ 6456681   ┆ Le Béage  ┆ A         ┆ ADM4      ┆ … ┆ 071       ┆ 07026     ┆ 9.547392  ┆ 5.257191 │\n",
      "└───────────┴───────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴──────────┘\n"
     ]
    }
   ],
   "source": [
    "def country_score(df: pl.LazyFrame) -> pl.LazyFrame:\n",
    "    return (\n",
    "        df.with_columns(\n",
    "            pop_multiplier=(1 + (pl.col(\"Population\").add(1).log10() / 10)),\n",
    "            area_multiplier=(1 + (pl.col(\"Area\").add(1).log10() / 20)),\n",
    "            feature_multiplier=pl.when(pl.col(\"feature_code\") == \"PCLI\")\n",
    "            .then(0.5)\n",
    "            .when(pl.col(\"feature_code\").str.contains(\"^PCL.*$\"))\n",
    "            .then(0.2)\n",
    "            .otherwise(0),\n",
    "        )\n",
    "        .with_columns(\n",
    "            adjusted_score_0=pl.col(\"score\")\n",
    "            * (\n",
    "                pl.col(\"pop_multiplier\")\n",
    "                + pl.col(\"area_multiplier\")\n",
    "                + pl.col(\"feature_multiplier\")\n",
    "            )\n",
    "        )\n",
    "        .sort(\"adjusted_score_0\", descending=True)\n",
    "        .select(\n",
    "            \"geonameId\",\n",
    "            \"name\",\n",
    "            cs.starts_with(\"admin\"),\n",
    "            cs.starts_with(\"adjusted_score\"),\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "def admin_score(\n",
    "    df: pl.LazyFrame, level: int, parent_weight: float = 0.3\n",
    ") -> pl.LazyFrame:\n",
    "    assert level in [1, 2, 3, 4], \"Level must be between 1 and 4\"\n",
    "    score_column = f\"adjusted_score_{level}\"\n",
    "    parent_score_column = f\"adjusted_score_{level - 1}\"\n",
    "\n",
    "    df = df.with_columns(\n",
    "        pop_multiplier=(1 + (pl.col(\"population\").add(1).log10() / 10)),\n",
    "    )\n",
    "    # Check all possible parent score columns, from highest to lowest\n",
    "    # This allows using any available parent score, not just the immediate parent\n",
    "    available_parent_scores = []\n",
    "    for parent_level in range(level - 1, -1, -1):\n",
    "        parent_score_column = f\"adjusted_score_{parent_level}\"\n",
    "        if parent_score_column in df.collect_schema().keys():\n",
    "            logger.debug(f\"Found parent score column: {parent_score_column}\")\n",
    "            available_parent_scores.append((parent_level, parent_score_column))\n",
    "            break  # Use the highest available parent level\n",
    "    if available_parent_scores:\n",
    "        # Use the highest available parent score\n",
    "        parent_level, parent_score_column = available_parent_scores[0]\n",
    "        logger.debug(\n",
    "            f\"Using parent score from level {parent_level}: {parent_score_column}\"\n",
    "        )\n",
    "\n",
    "        # Apply scoring with parent influence\n",
    "        df = df.with_columns(\n",
    "            (\n",
    "                (pl.col(\"score\") * pl.col(\"pop_multiplier\")).pow(1 - parent_weight)\n",
    "                * pl.col(parent_score_column).pow(parent_weight)\n",
    "            ).alias(score_column),\n",
    "        )\n",
    "    else:\n",
    "        # No parent scores available, use only the current score\n",
    "        logger.debug(\"No parent score columns found. Using only current level score.\")\n",
    "        df = df.with_columns(\n",
    "            (pl.col(\"score\") * pl.col(\"pop_multiplier\")).alias(score_column),\n",
    "        )\n",
    "    # Return sorted and selected columns\n",
    "    return df.sort(score_column, descending=True).select(\n",
    "        \"geonameId\",\n",
    "        \"name\",\n",
    "        cs.starts_with(\"admin\"),\n",
    "        cs.starts_with(\"adjusted_score\"),\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "def search_country(term: str, con: DuckDBPyConnection, limit: int = 20) -> pl.DataFrame:\n",
    "    \"\"\"Search for a country by name, ISO code, etc.\"\"\"\n",
    "    query = \"\"\"SELECT *,\n",
    "    -- High fixed score for exact matches\n",
    "    CASE\n",
    "        WHEN LOWER(ISO) = LOWER($term) THEN 10.0\n",
    "        WHEN LOWER(ISO3) = LOWER($term) THEN 8.0\n",
    "        WHEN LOWER(fips) = LOWER($term) THEN 4.0\n",
    "    END AS score\n",
    "    FROM admin0\n",
    "    WHERE\n",
    "        -- First exact match priority\n",
    "        LOWER(ISO) = LOWER($term) OR\n",
    "        LOWER(ISO3) = LOWER($term) OR\n",
    "        LOWER(fips) = LOWER($term)\n",
    "    UNION ALL\n",
    "    -- Then fall back to fuzzy search for anything not exact\n",
    "    SELECT * FROM (\n",
    "        SELECT *, fts_main_admin0.match_bm25(geonameId, $term) AS score\n",
    "        FROM admin0\n",
    "        WHERE\n",
    "            LOWER(ISO) != LOWER($term) AND\n",
    "            LOWER(ISO3) != LOWER($term) AND\n",
    "            LOWER(fips) != LOWER($term)\n",
    "    ) sq\n",
    "    WHERE score IS NOT NULL\n",
    "    ORDER BY score DESC\n",
    "    LIMIT $limit;\n",
    "    \"\"\"\n",
    "\n",
    "    results = con.execute(query, {\"term\": term, \"limit\": limit}).pl()\n",
    "\n",
    "    if not results.is_empty():\n",
    "        # Apply country-specific scoring\n",
    "        return results.lazy().pipe(country_score).collect()\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def build_path_conditions(df: pl.DataFrame, admin_cols: list[str]) -> str:\n",
    "    \"\"\"\n",
    "    Build SQL path conditions from unique admin code combinations.\n",
    "    \"\"\"\n",
    "    if not admin_cols:\n",
    "        return \"\"\n",
    "\n",
    "    # Extract only the relevant columns and remove rows with nulls\n",
    "    paths_df = df.select(admin_cols).drop_nulls()\n",
    "\n",
    "    if paths_df.is_empty():\n",
    "        return \"\"\n",
    "\n",
    "    # Get unique path combinations\n",
    "    unique_paths = paths_df.unique()\n",
    "\n",
    "    # Build OR conditions for each path\n",
    "    path_conditions = []\n",
    "    for row in unique_paths.iter_rows(named=True):\n",
    "        conditions = []\n",
    "        for col, val in row.items():\n",
    "            if val is None:\n",
    "                conditions.append(f\"{col} IS NULL\")\n",
    "            elif isinstance(val, str):\n",
    "                conditions.append(f\"{col} = '{val}'\")\n",
    "            else:\n",
    "                conditions.append(f\"{col} = {val}\")\n",
    "\n",
    "        if conditions:\n",
    "            path_conditions.append(f\"({' AND '.join(conditions)})\")\n",
    "\n",
    "    if path_conditions:\n",
    "        return \" OR \".join(path_conditions)\n",
    "\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def search_admin_level(\n",
    "    term: str,\n",
    "    level: int,\n",
    "    con: DuckDBPyConnection,\n",
    "    previous_results: pl.DataFrame | None = None,\n",
    "    limit: int = 20,\n",
    ") -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Search for a term with path-aware hierarchical filtering - simplified approach.\n",
    "    \"\"\"\n",
    "    assert level in [1, 2, 3, 4], \"Level must be between 1 and 4\"\n",
    "    table_name = f\"admin{level}\"\n",
    "\n",
    "    # Base query for finding matches\n",
    "    base_query = f\"\"\"\n",
    "    SELECT *, fts_main_{table_name}.match_bm25(geonameId, $term) AS score\n",
    "    FROM {table_name}\n",
    "    WHERE score IS NOT NULL\n",
    "    \"\"\"\n",
    "\n",
    "    # No filtering needed if no previous results\n",
    "    if previous_results is None or previous_results.is_empty():\n",
    "        query = f\"{base_query} ORDER BY score DESC LIMIT $limit\"\n",
    "        results = con.execute(query, {\"term\": term, \"limit\": limit}).pl()\n",
    "        return (\n",
    "            results.lazy().pipe(admin_score, level).collect()\n",
    "            if not results.is_empty()\n",
    "            else results\n",
    "        )\n",
    "\n",
    "    # Find the highest level we've searched so far\n",
    "    highest_level = -1\n",
    "    for i in range(level):\n",
    "        score_col = f\"adjusted_score_{i}\"\n",
    "        if score_col in previous_results.columns:\n",
    "            highest_level = i\n",
    "\n",
    "    if highest_level >= 0:\n",
    "        print(\n",
    "            f\"Highest searched level: {highest_level} (with '{f'adjusted_score_{highest_level}'}')\"\n",
    "        )\n",
    "\n",
    "        # Extract the admin columns up to the highest searched level\n",
    "        admin_cols = [f\"admin{i}_code\" for i in range(highest_level + 1)]\n",
    "\n",
    "        try:\n",
    "            # Extract unique combinations as path filters\n",
    "            path_conditions = build_path_conditions(previous_results, admin_cols)\n",
    "\n",
    "            if path_conditions:\n",
    "                print(f\"Filtering with path conditions: {path_conditions}\")\n",
    "\n",
    "                # Add path filtering to query\n",
    "                query = f\"\"\"\n",
    "                SELECT * FROM ({base_query})\n",
    "                WHERE {path_conditions}\n",
    "                ORDER BY score DESC\n",
    "                LIMIT $limit\n",
    "                \"\"\"\n",
    "\n",
    "                results = con.execute(query, {\"term\": term, \"limit\": limit}).pl()\n",
    "\n",
    "                # If filtering returned results, process them\n",
    "                if not results.is_empty():\n",
    "                    # Add parent score if available\n",
    "                    parent_score_col = f\"adjusted_score_{highest_level}\"\n",
    "                    if parent_score_col in previous_results.columns:\n",
    "                        # For simplicity, use the highest score for each path\n",
    "                        # This is a simplification but avoids complex joins\n",
    "                        max_scores = {}\n",
    "                        for row in (\n",
    "                            previous_results.select(admin_cols + [parent_score_col])\n",
    "                            .unique()\n",
    "                            .iter_rows(named=True)\n",
    "                        ):\n",
    "                            path_key = tuple(row[col] for col in admin_cols)\n",
    "                            max_scores[path_key] = row[parent_score_col]\n",
    "\n",
    "                        # Add parent scores to results\n",
    "                        if max_scores:\n",
    "                            results = results.with_columns(\n",
    "                                pl.lit(list(max_scores.values())[0]).alias(\n",
    "                                    parent_score_col\n",
    "                                )\n",
    "                            )\n",
    "                            print(f\"Added parent score from {parent_score_col}\")\n",
    "                else:\n",
    "                    print(\"No results with path filtering, trying unfiltered search\")\n",
    "                    # Fallback to unfiltered search\n",
    "                    results = con.execute(\n",
    "                        f\"{base_query} ORDER BY score DESC LIMIT $limit\",\n",
    "                        {\"term\": term, \"limit\": limit},\n",
    "                    ).pl()\n",
    "            else:\n",
    "                print(\"No valid paths found, using unfiltered search\")\n",
    "                # No valid paths, use unfiltered search\n",
    "                results = con.execute(\n",
    "                    f\"{base_query} ORDER BY score DESC LIMIT $limit\",\n",
    "                    {\"term\": term, \"limit\": limit},\n",
    "                ).pl()\n",
    "        except Exception as e:\n",
    "            print(f\"Error building path filters: {e}\")\n",
    "            # Error fallback\n",
    "            results = con.execute(\n",
    "                f\"\"\"\n",
    "            SELECT * FROM {table_name}\n",
    "            WHERE LOWER(name) LIKE '%' || LOWER($term) || '%'\n",
    "            ORDER BY name\n",
    "            LIMIT $limit\n",
    "            \"\"\",\n",
    "                {\"term\": term, \"limit\": limit},\n",
    "            ).pl()\n",
    "    else:\n",
    "        # No previous levels searched, use basic search\n",
    "        print(\"No previous levels searched, using basic search\")\n",
    "        results = con.execute(\n",
    "            f\"{base_query} ORDER BY score DESC LIMIT $limit\",\n",
    "            {\"term\": term, \"limit\": limit},\n",
    "        ).pl()\n",
    "\n",
    "    # Apply scoring\n",
    "    if not results.is_empty():\n",
    "        return results.lazy().pipe(admin_score, level).collect()\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def hierarchical_search(\n",
    "    search_terms: list[str | None], con: DuckDBPyConnection, limit: int = 10\n",
    ") -> dict[str, pl.DataFrame]:\n",
    "    \"\"\"\n",
    "    Perform hierarchical geographic search across admin levels.\n",
    "\n",
    "    Args:\n",
    "        search_terms: List of search terms, ordered by admin level (country, admin1, admin2, etc.)\n",
    "        con: Database connection\n",
    "        limit: Maximum results to return per level\n",
    "\n",
    "    Returns:\n",
    "        Dictionary mapping level names to search results\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    last_results = None\n",
    "    # Ensure we have enough search terms\n",
    "    if not search_terms:\n",
    "        return results\n",
    "\n",
    "    # Pad search terms if fewer than 5 are provided\n",
    "    search_terms = search_terms + [None] * (5 - len(search_terms))\n",
    "\n",
    "    # Level 0: Country search\n",
    "    if search_terms[0]:\n",
    "        logger.debug(f\"Searching for country: '{search_terms[0]}'\")\n",
    "        country_results = search_country(search_terms[0], con, limit)\n",
    "        if not country_results.is_empty():\n",
    "            results[\"country\"] = country_results\n",
    "            last_results = country_results\n",
    "\n",
    "    # Level 1: Admin1 search\n",
    "    if search_terms[1]:\n",
    "        logger.debug(f\"Searching for admin1: '{search_terms[1]}'\")\n",
    "        admin1_results = search_admin_level(\n",
    "            search_terms[1], 1, con, last_results, limit\n",
    "        )\n",
    "        if not admin1_results.is_empty():\n",
    "            results[\"admin1\"] = admin1_results\n",
    "            last_results = admin1_results\n",
    "\n",
    "    # Level 2: Admin2 search\n",
    "    if search_terms[2]:\n",
    "        logger.debug(f\"Searching for admin2: '{search_terms[2]}'\")\n",
    "        admin2_results = search_admin_level(\n",
    "            search_terms[2], 2, con, last_results, limit\n",
    "        )\n",
    "        if not admin2_results.is_empty():\n",
    "            results[\"admin2\"] = admin2_results\n",
    "            last_results = admin2_results\n",
    "\n",
    "    # Level 3: Admin3 search\n",
    "    if search_terms[3]:\n",
    "        logger.debug(f\"Searching for admin3: '{search_terms[3]}'\")\n",
    "        admin3_results = search_admin_level(\n",
    "            search_terms[3], 3, con, last_results, limit\n",
    "        )\n",
    "        if not admin3_results.is_empty():\n",
    "            results[\"admin3\"] = admin3_results\n",
    "            last_results = admin3_results\n",
    "\n",
    "    # Level 4: Admin4 search\n",
    "    if search_terms[4]:\n",
    "        logger.debug(f\"Searching for admin4: '{search_terms[4]}'\")\n",
    "        admin4_results = search_admin_level(\n",
    "            search_terms[4], 4, con, last_results, limit\n",
    "        )\n",
    "        if not admin4_results.is_empty():\n",
    "            results[\"admin4\"] = admin4_results\n",
    "            last_results = admin4_results\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# Search through the admin hierarchy\n",
    "results = hierarchical_search(\n",
    "    search_terms=[\"FR\", \"Provence-Alpes-Côte d'Azur\", None, None, \"Le Lavandou\"],\n",
    "    con=con,\n",
    ")\n",
    "\n",
    "# Access results for each level\n",
    "if \"country\" in results:\n",
    "    logger.debug(\"Country results:\")\n",
    "    print(results[\"country\"])\n",
    "if \"admin1\" in results:\n",
    "    logger.debug(\"Admin1 results:\")\n",
    "    print(results[\"admin1\"])\n",
    "if \"admin2\" in results:\n",
    "    logger.debug(\n",
    "        \"Admin2 results:\",\n",
    "    )\n",
    "    print(results[\"admin2\"])\n",
    "if \"admin3\" in results:\n",
    "    logger.debug(\n",
    "        \"Admin3 results:\",\n",
    "    )\n",
    "    print(results[\"admin3\"])\n",
    "if \"admin4\" in results:\n",
    "    logger.debug(\n",
    "        \"Admin4 results:\",\n",
    "    )\n",
    "    print(results[\"admin4\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backfill_hierarchy(row: dict, con: DuckDBPyConnection) -> dict:\n",
    "    def get_where_clause(codes: list[str | None]) -> str:\n",
    "        return \"WHERE \" + \" AND \".join(\n",
    "            [\n",
    "                f\"admin{i}_code = '{code}'\"\n",
    "                if code is not None\n",
    "                else f\"admin{i}_code IS NULL\"\n",
    "                for i, code in enumerate(codes)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    hierarchy = {}\n",
    "    codes = []\n",
    "    for i in range(5):\n",
    "        code = row.get(f\"admin{i}_code\")\n",
    "        codes.append(code)\n",
    "        if code is not None:\n",
    "            df = con.execute(f\"\"\"\n",
    "                SELECT geonameId, name FROM admin{i}\n",
    "                {get_where_clause(codes)}\n",
    "                LIMIT 1\n",
    "            \"\"\").pl()\n",
    "            if not df.is_empty():\n",
    "                hierarchy[f\"admin{i}\"] = df.to_dicts()[0]\n",
    "    return hierarchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (10, 11)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>geonameId</th><th>name</th><th>feature_class</th><th>feature_code</th><th>admin0_code</th><th>admin1_code</th><th>admin2_code</th><th>admin3_code</th><th>admin4_code</th><th>adjusted_score_0</th><th>adjusted_score_4</th></tr><tr><td>i32</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>6615009</td><td>&quot;Le Lavandou&quot;</td><td>&quot;A&quot;</td><td>&quot;ADM4&quot;</td><td>&quot;FR&quot;</td><td>&quot;93&quot;</td><td>&quot;83&quot;</td><td>&quot;832&quot;</td><td>&quot;83070&quot;</td><td>35.694998</td><td>20.117537</td></tr><tr><td>6456500</td><td>&quot;Le Mesnil-le-Roi&quot;</td><td>&quot;A&quot;</td><td>&quot;ADM4&quot;</td><td>&quot;FR&quot;</td><td>&quot;11&quot;</td><td>&quot;78&quot;</td><td>&quot;783&quot;</td><td>&quot;78396&quot;</td><td>35.694998</td><td>8.56702</td></tr><tr><td>6614571</td><td>&quot;Le Tréport&quot;</td><td>&quot;A&quot;</td><td>&quot;ADM4&quot;</td><td>&quot;FR&quot;</td><td>&quot;28&quot;</td><td>&quot;76&quot;</td><td>&quot;761&quot;</td><td>&quot;76711&quot;</td><td>35.694998</td><td>8.363143</td></tr><tr><td>6457137</td><td>&quot;Le Boupère&quot;</td><td>&quot;A&quot;</td><td>&quot;ADM4&quot;</td><td>&quot;FR&quot;</td><td>&quot;52&quot;</td><td>&quot;85&quot;</td><td>&quot;851&quot;</td><td>&quot;85031&quot;</td><td>35.694998</td><td>8.279699</td></tr><tr><td>6456350</td><td>&quot;Le Perréon&quot;</td><td>&quot;A&quot;</td><td>&quot;ADM4&quot;</td><td>&quot;FR&quot;</td><td>&quot;84&quot;</td><td>&quot;69&quot;</td><td>&quot;692&quot;</td><td>&quot;69151&quot;</td><td>35.694998</td><td>8.241325</td></tr><tr><td>6455915</td><td>&quot;Le Pêchereau&quot;</td><td>&quot;A&quot;</td><td>&quot;ADM4&quot;</td><td>&quot;FR&quot;</td><td>&quot;24&quot;</td><td>&quot;36&quot;</td><td>&quot;362&quot;</td><td>&quot;36154&quot;</td><td>35.694998</td><td>8.182559</td></tr><tr><td>6455838</td><td>&quot;Le Tréhou&quot;</td><td>&quot;A&quot;</td><td>&quot;ADM4&quot;</td><td>&quot;FR&quot;</td><td>&quot;53&quot;</td><td>&quot;29&quot;</td><td>&quot;291&quot;</td><td>&quot;29294&quot;</td><td>35.694998</td><td>7.978513</td></tr><tr><td>6616109</td><td>&quot;Le Faouët&quot;</td><td>&quot;A&quot;</td><td>&quot;ADM4&quot;</td><td>&quot;FR&quot;</td><td>&quot;53&quot;</td><td>&quot;22&quot;</td><td>&quot;222&quot;</td><td>&quot;22057&quot;</td><td>35.694998</td><td>7.978342</td></tr><tr><td>6456394</td><td>&quot;Le Rousset&quot;</td><td>&quot;A&quot;</td><td>&quot;ADM4H&quot;</td><td>&quot;FR&quot;</td><td>&quot;27&quot;</td><td>&quot;71&quot;</td><td>&quot;713&quot;</td><td>&quot;71375&quot;</td><td>35.694998</td><td>7.835095</td></tr><tr><td>6455885</td><td>&quot;Le Soulié&quot;</td><td>&quot;A&quot;</td><td>&quot;ADM4&quot;</td><td>&quot;FR&quot;</td><td>&quot;76&quot;</td><td>&quot;34&quot;</td><td>&quot;341&quot;</td><td>&quot;34305&quot;</td><td>35.694998</td><td>7.673995</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (10, 11)\n",
       "┌───────────┬───────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬──────────┐\n",
       "│ geonameId ┆ name      ┆ feature_c ┆ feature_c ┆ … ┆ admin3_co ┆ admin4_co ┆ adjusted_ ┆ adjusted │\n",
       "│ ---       ┆ ---       ┆ lass      ┆ ode       ┆   ┆ de        ┆ de        ┆ score_0   ┆ _score_4 │\n",
       "│ i32       ┆ str       ┆ ---       ┆ ---       ┆   ┆ ---       ┆ ---       ┆ ---       ┆ ---      │\n",
       "│           ┆           ┆ str       ┆ str       ┆   ┆ str       ┆ str       ┆ f64       ┆ f64      │\n",
       "╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪══════════╡\n",
       "│ 6615009   ┆ Le        ┆ A         ┆ ADM4      ┆ … ┆ 832       ┆ 83070     ┆ 35.694998 ┆ 20.11753 │\n",
       "│           ┆ Lavandou  ┆           ┆           ┆   ┆           ┆           ┆           ┆ 7        │\n",
       "│ 6456500   ┆ Le Mesnil ┆ A         ┆ ADM4      ┆ … ┆ 783       ┆ 78396     ┆ 35.694998 ┆ 8.56702  │\n",
       "│           ┆ -le-Roi   ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ 6614571   ┆ Le        ┆ A         ┆ ADM4      ┆ … ┆ 761       ┆ 76711     ┆ 35.694998 ┆ 8.363143 │\n",
       "│           ┆ Tréport   ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ 6457137   ┆ Le        ┆ A         ┆ ADM4      ┆ … ┆ 851       ┆ 85031     ┆ 35.694998 ┆ 8.279699 │\n",
       "│           ┆ Boupère   ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ 6456350   ┆ Le        ┆ A         ┆ ADM4      ┆ … ┆ 692       ┆ 69151     ┆ 35.694998 ┆ 8.241325 │\n",
       "│           ┆ Perréon   ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ 6455915   ┆ Le        ┆ A         ┆ ADM4      ┆ … ┆ 362       ┆ 36154     ┆ 35.694998 ┆ 8.182559 │\n",
       "│           ┆ Pêchereau ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ 6455838   ┆ Le Tréhou ┆ A         ┆ ADM4      ┆ … ┆ 291       ┆ 29294     ┆ 35.694998 ┆ 7.978513 │\n",
       "│ 6616109   ┆ Le Faouët ┆ A         ┆ ADM4      ┆ … ┆ 222       ┆ 22057     ┆ 35.694998 ┆ 7.978342 │\n",
       "│ 6456394   ┆ Le        ┆ A         ┆ ADM4H     ┆ … ┆ 713       ┆ 71375     ┆ 35.694998 ┆ 7.835095 │\n",
       "│           ┆ Rousset   ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ 6455885   ┆ Le Soulié ┆ A         ┆ ADM4      ┆ … ┆ 341       ┆ 34305     ┆ 35.694998 ┆ 7.673995 │\n",
       "└───────────┴───────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴──────────┘"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[\"admin4\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-04-29 14:48:09.682\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mhierarchical_search\u001b[0m:\u001b[36m333\u001b[0m - \u001b[34m\u001b[1mSearching for country: 'FR'\u001b[0m\n",
      "\u001b[32m2025-04-29 14:48:09.700\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mhierarchical_search\u001b[0m:\u001b[36m371\u001b[0m - \u001b[34m\u001b[1mSearching for admin4: 'Le Lavandou'\u001b[0m\n",
      "\u001b[32m2025-04-29 14:48:09.761\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36madmin_score\u001b[0m:\u001b[36m48\u001b[0m - \u001b[34m\u001b[1mFound parent score column: adjusted_score_0\u001b[0m\n",
      "\u001b[32m2025-04-29 14:48:09.761\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36madmin_score\u001b[0m:\u001b[36m54\u001b[0m - \u001b[34m\u001b[1mUsing parent score from level 0: adjusted_score_0\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Highest searched level: 0 (with 'adjusted_score_0')\n",
      "Filtering with path conditions: (admin0_code = 'FR') OR (admin0_code = 'PF') OR (admin0_code = 'GF') OR (admin0_code = 'MF')\n",
      "Added parent score from adjusted_score_0\n",
      "{'adjusted_score_0': 35.69499808597985,\n",
      " 'adjusted_score_4': 20.11753652073296,\n",
      " 'admin0_code': 'FR',\n",
      " 'admin1_code': '93',\n",
      " 'admin2_code': '83',\n",
      " 'admin3_code': '832',\n",
      " 'admin4_code': '83070',\n",
      " 'feature_class': 'A',\n",
      " 'feature_code': 'ADM4',\n",
      " 'geonameId': 6615009,\n",
      " 'name': 'Le Lavandou'}\n",
      "{'admin0': {'geonameId': 3017382, 'name': 'Republic of France'},\n",
      " 'admin1': {'geonameId': 2985244, 'name': \"Provence-Alpes-Côte d'Azur\"},\n",
      " 'admin2': {'geonameId': 2970749, 'name': 'Var'},\n",
      " 'admin3': {'geonameId': 2972326, 'name': 'Arrondissement de Toulon'},\n",
      " 'admin4': {'geonameId': 6615009, 'name': 'Le Lavandou'}}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "\n",
    "results = hierarchical_search(\n",
    "    search_terms=[\"FR\", None, None, None, \"Le Lavandou\"], con=con\n",
    ")\n",
    "row = results[\"admin4\"].row(0, named=True)\n",
    "pprint(row)\n",
    "\n",
    "pprint(backfill_hierarchy(row, con))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-04-29 14:47:28.843\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mhierarchical_search\u001b[0m:\u001b[36m341\u001b[0m - \u001b[34m\u001b[1mSearching for admin1: 'FL'\u001b[0m\n",
      "\u001b[32m2025-04-29 14:47:28.867\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36madmin_score\u001b[0m:\u001b[36m67\u001b[0m - \u001b[34m\u001b[1mNo parent score columns found. Using only current level score.\u001b[0m\n",
      "\u001b[32m2025-04-29 14:47:28.868\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mhierarchical_search\u001b[0m:\u001b[36m361\u001b[0m - \u001b[34m\u001b[1mSearching for admin3: 'Lakeland'\u001b[0m\n",
      "\u001b[32m2025-04-29 14:47:28.902\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36madmin_score\u001b[0m:\u001b[36m48\u001b[0m - \u001b[34m\u001b[1mFound parent score column: adjusted_score_1\u001b[0m\n",
      "\u001b[32m2025-04-29 14:47:28.902\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36madmin_score\u001b[0m:\u001b[36m54\u001b[0m - \u001b[34m\u001b[1mUsing parent score from level 1: adjusted_score_1\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Highest searched level: 1 (with 'adjusted_score_1')\n",
      "Filtering with path conditions: (admin0_code = 'US' AND admin1_code = 'FL') OR (admin0_code = 'NL' AND admin1_code = '16')\n",
      "Added parent score from adjusted_score_1\n",
      "{'adjusted_score_1': 5.956702527169346,\n",
      " 'adjusted_score_3': 8.674242603390644,\n",
      " 'admin0_code': 'US',\n",
      " 'admin1_code': 'FL',\n",
      " 'admin2_code': '105',\n",
      " 'admin3_code': '7170309',\n",
      " 'admin4_code': None,\n",
      " 'feature_class': 'A',\n",
      " 'feature_code': 'ADM3',\n",
      " 'geonameId': 7170309,\n",
      " 'name': 'City of Lakeland'}\n",
      "{'admin0': {'geonameId': 6252001, 'name': 'United States'},\n",
      " 'admin1': {'geonameId': 4155751, 'name': 'Florida'},\n",
      " 'admin2': {'geonameId': 4168988, 'name': 'Polk County'},\n",
      " 'admin3': {'geonameId': 7170309, 'name': 'City of Lakeland'}}\n"
     ]
    }
   ],
   "source": [
    "results = hierarchical_search(\n",
    "    search_terms=[None, \"FL\", None, \"Lakeland\", None], con=con\n",
    ")\n",
    "row = results[\"admin3\"].row(0, named=True)\n",
    "\n",
    "\n",
    "pprint(row)\n",
    "\n",
    "pprint(backfill_hierarchy(row, con))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-04-29 14:41:52.225\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mhierarchical_search\u001b[0m:\u001b[36m333\u001b[0m - \u001b[34m\u001b[1mSearching for country: 'FR'\u001b[0m\n",
      "\u001b[32m2025-04-29 14:41:52.239\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mhierarchical_search\u001b[0m:\u001b[36m341\u001b[0m - \u001b[34m\u001b[1mSearching for admin1: 'Provence-Alpes-Côte d'Azur'\u001b[0m\n",
      "\u001b[32m2025-04-29 14:41:52.265\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36madmin_score\u001b[0m:\u001b[36m48\u001b[0m - \u001b[34m\u001b[1mFound parent score column: adjusted_score_0\u001b[0m\n",
      "\u001b[32m2025-04-29 14:41:52.266\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36madmin_score\u001b[0m:\u001b[36m54\u001b[0m - \u001b[34m\u001b[1mUsing parent score from level 0: adjusted_score_0\u001b[0m\n",
      "\u001b[32m2025-04-29 14:41:52.267\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mhierarchical_search\u001b[0m:\u001b[36m351\u001b[0m - \u001b[34m\u001b[1mSearching for admin2: 'Var'\u001b[0m\n",
      "\u001b[32m2025-04-29 14:41:52.286\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36madmin_score\u001b[0m:\u001b[36m48\u001b[0m - \u001b[34m\u001b[1mFound parent score column: adjusted_score_1\u001b[0m\n",
      "\u001b[32m2025-04-29 14:41:52.287\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36madmin_score\u001b[0m:\u001b[36m54\u001b[0m - \u001b[34m\u001b[1mUsing parent score from level 1: adjusted_score_1\u001b[0m\n",
      "\u001b[32m2025-04-29 14:41:52.289\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mhierarchical_search\u001b[0m:\u001b[36m361\u001b[0m - \u001b[34m\u001b[1mSearching for admin3: 'Arrondissement de Toulon'\u001b[0m\n",
      "\u001b[32m2025-04-29 14:41:52.327\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36madmin_score\u001b[0m:\u001b[36m48\u001b[0m - \u001b[34m\u001b[1mFound parent score column: adjusted_score_2\u001b[0m\n",
      "\u001b[32m2025-04-29 14:41:52.328\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36madmin_score\u001b[0m:\u001b[36m54\u001b[0m - \u001b[34m\u001b[1mUsing parent score from level 2: adjusted_score_2\u001b[0m\n",
      "\u001b[32m2025-04-29 14:41:52.329\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mhierarchical_search\u001b[0m:\u001b[36m371\u001b[0m - \u001b[34m\u001b[1mSearching for admin4: 'Le Lavandou'\u001b[0m\n",
      "\u001b[32m2025-04-29 14:41:52.361\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36madmin_score\u001b[0m:\u001b[36m48\u001b[0m - \u001b[34m\u001b[1mFound parent score column: adjusted_score_3\u001b[0m\n",
      "\u001b[32m2025-04-29 14:41:52.362\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36madmin_score\u001b[0m:\u001b[36m54\u001b[0m - \u001b[34m\u001b[1mUsing parent score from level 3: adjusted_score_3\u001b[0m\n",
      "\u001b[32m2025-04-29 14:41:52.362\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m15\u001b[0m - \u001b[34m\u001b[1mCountry results:\u001b[0m\n",
      "\u001b[32m2025-04-29 14:41:52.363\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m17\u001b[0m - \u001b[34m\u001b[1mAdmin1 results:\u001b[0m\n",
      "\u001b[32m2025-04-29 14:41:52.363\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m19\u001b[0m - \u001b[34m\u001b[1mAdmin2 results:\u001b[0m\n",
      "\u001b[32m2025-04-29 14:41:52.363\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m21\u001b[0m - \u001b[34m\u001b[1mAdmin3 results:\u001b[0m\n",
      "\u001b[32m2025-04-29 14:41:52.364\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m23\u001b[0m - \u001b[34m\u001b[1mAdmin4 results:\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Highest searched level: 0 (with 'adjusted_score_0')\n",
      "Filtering with path conditions: (admin0_code = 'PF') OR (admin0_code = 'GF') OR (admin0_code = 'MF') OR (admin0_code = 'FR')\n",
      "Added parent score from adjusted_score_0\n",
      "Highest searched level: 1 (with 'adjusted_score_1')\n",
      "Filtering with path conditions: (admin0_code = 'FR' AND admin1_code = 'B9') OR (admin0_code = 'FR' AND admin1_code = '24') OR (admin0_code = 'FR' AND admin1_code = '32') OR (admin0_code = 'FR' AND admin1_code = '52') OR (admin0_code = 'FR' AND admin1_code = 'B4') OR (admin0_code = 'FR' AND admin1_code = '11') OR (admin0_code = 'FR' AND admin1_code = '93') OR (admin0_code = 'FR' AND admin1_code = '84')\n",
      "Added parent score from adjusted_score_1\n",
      "Highest searched level: 2 (with 'adjusted_score_2')\n",
      "Filtering with path conditions: (admin0_code = 'FR' AND admin1_code = '93' AND admin2_code = '83')\n",
      "Added parent score from adjusted_score_2\n",
      "Highest searched level: 3 (with 'adjusted_score_3')\n",
      "Filtering with path conditions: (admin0_code = 'FR' AND admin1_code = '93' AND admin2_code = '83' AND admin3_code = '831') OR (admin0_code = 'FR' AND admin1_code = '93' AND admin2_code = '83' AND admin3_code = '833') OR (admin0_code = 'FR' AND admin1_code = '93' AND admin2_code = '83' AND admin3_code = '832')\n",
      "Added parent score from adjusted_score_3\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (10, 11)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>geonameId</th><th>name</th><th>feature_class</th><th>feature_code</th><th>admin0_code</th><th>admin1_code</th><th>admin2_code</th><th>admin3_code</th><th>admin4_code</th><th>adjusted_score_3</th><th>adjusted_score_4</th></tr><tr><td>i32</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>6615009</td><td>&quot;Le Lavandou&quot;</td><td>&quot;A&quot;</td><td>&quot;ADM4&quot;</td><td>&quot;FR&quot;</td><td>&quot;93&quot;</td><td>&quot;83&quot;</td><td>&quot;832&quot;</td><td>&quot;83070&quot;</td><td>17.210859</td><td>16.163424</td></tr><tr><td>6457122</td><td>&quot;Le Luc&quot;</td><td>&quot;A&quot;</td><td>&quot;ADM4&quot;</td><td>&quot;FR&quot;</td><td>&quot;93&quot;</td><td>&quot;83&quot;</td><td>&quot;833&quot;</td><td>&quot;83073&quot;</td><td>17.210859</td><td>6.716323</td></tr><tr><td>6457128</td><td>&quot;Le Pradet&quot;</td><td>&quot;A&quot;</td><td>&quot;ADM4&quot;</td><td>&quot;FR&quot;</td><td>&quot;93&quot;</td><td>&quot;83&quot;</td><td>&quot;832&quot;</td><td>&quot;83098&quot;</td><td>17.210859</td><td>6.703486</td></tr><tr><td>6457127</td><td>&quot;Le Muy&quot;</td><td>&quot;A&quot;</td><td>&quot;ADM4&quot;</td><td>&quot;FR&quot;</td><td>&quot;93&quot;</td><td>&quot;83&quot;</td><td>&quot;831&quot;</td><td>&quot;83086&quot;</td><td>17.210859</td><td>6.691712</td></tr><tr><td>6456550</td><td>&quot;Le Val&quot;</td><td>&quot;A&quot;</td><td>&quot;ADM4&quot;</td><td>&quot;FR&quot;</td><td>&quot;93&quot;</td><td>&quot;83&quot;</td><td>&quot;833&quot;</td><td>&quot;83143&quot;</td><td>17.210859</td><td>6.579679</td></tr><tr><td>6456549</td><td>&quot;Le Thoronet&quot;</td><td>&quot;A&quot;</td><td>&quot;ADM4&quot;</td><td>&quot;FR&quot;</td><td>&quot;93&quot;</td><td>&quot;83&quot;</td><td>&quot;833&quot;</td><td>&quot;83136&quot;</td><td>17.210859</td><td>6.49699</td></tr><tr><td>6457113</td><td>&quot;Le Beausset&quot;</td><td>&quot;A&quot;</td><td>&quot;ADM4&quot;</td><td>&quot;FR&quot;</td><td>&quot;93&quot;</td><td>&quot;83&quot;</td><td>&quot;832&quot;</td><td>&quot;83016&quot;</td><td>17.210859</td><td>6.46819</td></tr><tr><td>6614863</td><td>&quot;Le Bourguet&quot;</td><td>&quot;A&quot;</td><td>&quot;ADM4&quot;</td><td>&quot;FR&quot;</td><td>&quot;93&quot;</td><td>&quot;83&quot;</td><td>&quot;831&quot;</td><td>&quot;83020&quot;</td><td>17.210859</td><td>5.842643</td></tr><tr><td>6457115</td><td>&quot;Le Cannet-des-Maures&quot;</td><td>&quot;A&quot;</td><td>&quot;ADM4&quot;</td><td>&quot;FR&quot;</td><td>&quot;93&quot;</td><td>&quot;83&quot;</td><td>&quot;833&quot;</td><td>&quot;83031&quot;</td><td>17.210859</td><td>5.841804</td></tr><tr><td>6456545</td><td>&quot;Le Revest-les-Eaux&quot;</td><td>&quot;A&quot;</td><td>&quot;ADM4&quot;</td><td>&quot;FR&quot;</td><td>&quot;93&quot;</td><td>&quot;83&quot;</td><td>&quot;832&quot;</td><td>&quot;83103&quot;</td><td>17.210859</td><td>5.655776</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (10, 11)\n",
       "┌───────────┬───────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬──────────┐\n",
       "│ geonameId ┆ name      ┆ feature_c ┆ feature_c ┆ … ┆ admin3_co ┆ admin4_co ┆ adjusted_ ┆ adjusted │\n",
       "│ ---       ┆ ---       ┆ lass      ┆ ode       ┆   ┆ de        ┆ de        ┆ score_3   ┆ _score_4 │\n",
       "│ i32       ┆ str       ┆ ---       ┆ ---       ┆   ┆ ---       ┆ ---       ┆ ---       ┆ ---      │\n",
       "│           ┆           ┆ str       ┆ str       ┆   ┆ str       ┆ str       ┆ f64       ┆ f64      │\n",
       "╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪══════════╡\n",
       "│ 6615009   ┆ Le        ┆ A         ┆ ADM4      ┆ … ┆ 832       ┆ 83070     ┆ 17.210859 ┆ 16.16342 │\n",
       "│           ┆ Lavandou  ┆           ┆           ┆   ┆           ┆           ┆           ┆ 4        │\n",
       "│ 6457122   ┆ Le Luc    ┆ A         ┆ ADM4      ┆ … ┆ 833       ┆ 83073     ┆ 17.210859 ┆ 6.716323 │\n",
       "│ 6457128   ┆ Le Pradet ┆ A         ┆ ADM4      ┆ … ┆ 832       ┆ 83098     ┆ 17.210859 ┆ 6.703486 │\n",
       "│ 6457127   ┆ Le Muy    ┆ A         ┆ ADM4      ┆ … ┆ 831       ┆ 83086     ┆ 17.210859 ┆ 6.691712 │\n",
       "│ 6456550   ┆ Le Val    ┆ A         ┆ ADM4      ┆ … ┆ 833       ┆ 83143     ┆ 17.210859 ┆ 6.579679 │\n",
       "│ 6456549   ┆ Le        ┆ A         ┆ ADM4      ┆ … ┆ 833       ┆ 83136     ┆ 17.210859 ┆ 6.49699  │\n",
       "│           ┆ Thoronet  ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ 6457113   ┆ Le        ┆ A         ┆ ADM4      ┆ … ┆ 832       ┆ 83016     ┆ 17.210859 ┆ 6.46819  │\n",
       "│           ┆ Beausset  ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ 6614863   ┆ Le        ┆ A         ┆ ADM4      ┆ … ┆ 831       ┆ 83020     ┆ 17.210859 ┆ 5.842643 │\n",
       "│           ┆ Bourguet  ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ 6457115   ┆ Le Cannet ┆ A         ┆ ADM4      ┆ … ┆ 833       ┆ 83031     ┆ 17.210859 ┆ 5.841804 │\n",
       "│           ┆ -des-Maur ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ es        ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ 6456545   ┆ Le Revest ┆ A         ┆ ADM4      ┆ … ┆ 832       ┆ 83103     ┆ 17.210859 ┆ 5.655776 │\n",
       "│           ┆ -les-Eaux ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "└───────────┴───────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴──────────┘"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Search through the admin hierarchy\n",
    "results = hierarchical_search(\n",
    "    search_terms=[\n",
    "        \"FR\",\n",
    "        \"Provence-Alpes-Côte d'Azur\",\n",
    "        \"Var\",\n",
    "        \"Arrondissement de Toulon\",\n",
    "        \"Le Lavandou\",\n",
    "    ],\n",
    "    con=con,\n",
    ")\n",
    "\n",
    "# Access results for each level\n",
    "if \"country\" in results:\n",
    "    logger.debug(\"Country results:\", results[\"country\"])\n",
    "if \"admin1\" in results:\n",
    "    logger.debug(\"Admin1 results:\", results[\"admin1\"])\n",
    "if \"admin2\" in results:\n",
    "    logger.debug(\"Admin2 results:\", results[\"admin2\"])\n",
    "if \"admin3\" in results:\n",
    "    logger.debug(\"Admin3 results:\", results[\"admin3\"])\n",
    "if \"admin4\" in results:\n",
    "    logger.debug(\"Admin4 results:\", results[\"admin4\"])\n",
    "results[\"admin4\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = (\n",
    "    con.execute(\"SELECT geonameId, latitude, longitude FROM allCountries\")\n",
    "    .pl()\n",
    "    .select(\n",
    "        pl.col(\"geonameId\"),\n",
    "        pl.concat_list(pl.col(\"latitude\"), pl.col(\"longitude\"))\n",
    "        .cast(pl.Array(pl.Float32, 2))\n",
    "        .alias(\"vectors\"),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_coordinates1 = np.array([51.549902, -0.121696], dtype=np.float32)\n",
    "my_coordinates2 = np.array([37.77493, -122.41942], dtype=np.float32)\n",
    "\n",
    "vidx = VectorIndex(\"latlon\", data, metric=\"haversine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (path := Path(\"./data/processed/latlon.index\")).exists():\n",
    "    logger.debug(\"Loading index...\")\n",
    "    index = Index.restore(path, view=True)\n",
    "    if index is None:\n",
    "        raise ValueError(\"Failed to load index\")\n",
    "else:\n",
    "    logger.debug(\"Creating index...\")\n",
    "    coordinates = df.select([\"latitude\", \"longitude\"]).to_numpy(order=\"c\")\n",
    "    labels = df[\"geonameId\"].to_numpy()\n",
    "    index: Index = Index(ndim=2, metric=\"haversine\", dtype=\"f32\")\n",
    "    index.add(keys=labels, vectors=coordinates, log=True)\n",
    "    index.save(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example function to search and return results with distances\n",
    "def search_with_distances(\n",
    "    index: Index,\n",
    "    my_coordinates: NDArray[np.float32],\n",
    "    original_df: pl.LazyFrame,\n",
    "    k=10,\n",
    "    exact=False,\n",
    "):\n",
    "    # Perform the search\n",
    "    output = index.search(vectors=my_coordinates, count=k, log=True, exact=exact)\n",
    "\n",
    "    logger.debug(f\"Visited members: {output.visited_members}\")\n",
    "    logger.debug(f\"Computed distances: {output.computed_distances}\")\n",
    "\n",
    "    # Extract keys (geonameids) and distances\n",
    "    keys = output.keys\n",
    "    distances = output.distances\n",
    "\n",
    "    # Create a DataFrame from the search results\n",
    "    results_df = pl.LazyFrame(\n",
    "        data={\"geonameId\": keys, \"distance\": distances},\n",
    "        schema={\"geonameId\": pl.UInt32, \"distance\": pl.Float32},\n",
    "    ).with_columns(pl.col(\"distance\") * 6371.0)\n",
    "\n",
    "    # Join the results with the original DataFrame to get detailed information\n",
    "    detailed_results_df = results_df.join(original_df, on=\"geonameId\", how=\"left\")\n",
    "\n",
    "    # Sort by distance\n",
    "    sorted_results_df = detailed_results_df.sort(\"distance\")\n",
    "\n",
    "    return sorted_results_df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_with_distances(index, my_coordinates2, df.lazy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output: Matches = index.search(vectors=my_coordinates1, count=10, log=True)\n",
    "logger.debug(f\"{output.computed_distances=}\")\n",
    "logger.debug(f\"{output.visited_members=}\")\n",
    "df.filter(pl.col(\"geonameId\").is_in(output.keys))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
